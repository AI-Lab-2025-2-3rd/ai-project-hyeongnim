{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Lab-2025-2-3rd/ai-project-hyeongnim/blob/Do0o0ony/%EC%98%88%EC%95%84~.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JXf8zejTrcn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/KUBIG/DL/KUBIG_25-W STUDY/제출용/data\""
      ],
      "metadata": {
        "id": "aN2arB-sT0PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading"
      ],
      "metadata": {
        "id": "HW_kMPiYT5Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원본 데이터 : <br>\n",
        "TRAIN = pd.read_csv(path+\"/train.csv\")<br>\n",
        "TEST = pd.read_csv(path+\"/test.csv\")<br>\n",
        "\n",
        "- 원본 데이터 전처리 : <br>\n",
        "TRAIN_prep = pd.read_csv(path+\"/TRAIN_prep.csv\")<br>\n",
        "TEST_prep = pd.read_csv(path+\"/TEST_prep.csv\")<br>\n",
        "\n",
        "\n",
        "- train/valid split : <br>\n",
        "train_model = pd.read_csv(path+\"/train_model.csv\") <br>\n",
        "valid_model = pd.read_csv(path+\"/valid_model.csv\") <br>\n",
        "\n",
        "- train/valid split : <br>\n",
        "train_prep = pd.read_csv(path+\"/train_prep.csv\") <br>\n",
        "valid_prep = pd.read_csv(path+\"/valid_prep.csv\") <br>"
      ],
      "metadata": {
        "id": "fbkFjEX9bddC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "TRAIN = pd.read_csv(path+\"/train.csv\")\n",
        "TEST = pd.read_csv(path+\"/test.csv\")"
      ],
      "metadata": {
        "id": "Gvva3dCyT6ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing + Encoding"
      ],
      "metadata": {
        "id": "IEGxHBhhTxCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2NY_C7-2T8E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "\n",
        "    start_code = 0xAC00 # 한글 시작 유니코드\n",
        "    end_code = 0xD7A3 # 한글 끝 유니코드\n",
        "\n",
        "    # 초성, 중성, 종성 리스트\n",
        "    cho = [chr(i) for i in range(0x1100, 0x1113)] # 초성 유니코드\n",
        "    jung = [chr(i) for i in range(0x1161, 0x1176)] # 중성 유니코드\n",
        "    jong = [chr(i) for i in range(0x11A8, 0x11C3)] # 종성 유니코드\n",
        "\n",
        "    chos = []\n",
        "    jungs = []\n",
        "    jongs = []\n",
        "    positions = []\n",
        "\n",
        "    cleaned_text = re.sub(r'[^\\uAC00-\\uD7A3\\s]', '', text)  # 한글과 공백만 남기기\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # 중복 공백 제거\n",
        "\n",
        "\n",
        "    for char in cleaned_text:\n",
        "        if start_code <= ord(char) <= end_code:\n",
        "            char_code = ord(char) - start_code\n",
        "            cho_idx = char_code // (21 * 28)\n",
        "            jung_idx = (char_code % (21 * 28)) // 28\n",
        "            jong_idx = char_code % 28\n",
        "\n",
        "            # 초성, 중성\n",
        "            chos.append(cho[cho_idx])\n",
        "            jungs.append(jung[jung_idx])\n",
        "\n",
        "            # 종성 (없을 경우 'ㅉ'토큰; 받침에 올 수 없는 쌍자음)\n",
        "            jongs.append(jong[jong_idx - 1] if jong_idx > 0 else 'ㅉ')\n",
        "        else:\n",
        "            # 한글이 아닌 경우는 무시\n",
        "            continue\n",
        "\n",
        "    word_list = cleaned_text.split()  # 띄어쓰기 기준으로 단어 분리\n",
        "    for word in word_list:\n",
        "        for idx, _ in enumerate(word):\n",
        "            relative_pos = (idx + 1) / len(word)  # 현재 음절의 상대적 위치 계산\n",
        "            positions.append(round(relative_pos,2))\n",
        "\n",
        "    return chos, jungs, jongs, positions\n",
        "\n",
        "def extract_kr_char(text):\n",
        "    kr_chars = re.findall(r'[가-힣]', text)\n",
        "    return kr_chars\n",
        "\n",
        "\n",
        "def Encoder(data, jamo_to_index):\n",
        "  encoded_data = []\n",
        "  for jamos in data:\n",
        "    index_sequences = []\n",
        "    for jamo in jamos:\n",
        "      index_sequences.append(jamo_to_index[jamo])\n",
        "    encoded_data.append(index_sequences)\n",
        "  return encoded_data\n",
        "\n",
        "def preprocessing_encoding(df, train_flg = True):\n",
        "  df_prep = pd.DataFrame()\n",
        "  data = df.copy()\n",
        "\n",
        "  # 초성, 중성, 종성 유니코드 범위\n",
        "  cho = [chr(i) for i in range(0x1100, 0x1113)]  # 초성 (ㄱ~ㅎ)\n",
        "  jung = [chr(i) for i in range(0x1161, 0x1176)]  # 중성 (ㅏ~ㅣ)\n",
        "  jong = [chr(i) for i in range(0x11A8, 0x11C3)]  # 종성 (ㄱ~ㅄ)\n",
        "  jamo = cho + jung + jong + ['ㅉ']\n",
        "\n",
        "  jamo_to_index = {jamo[i]:i for i in range(len(jamo))}\n",
        "  index_to_jamo = {i:jamo[i] for i in range(len(jamo))}\n",
        "\n",
        "  for i in tqdm(range(len(data))):\n",
        "    chos, jungs, jongs, poss = preprocessing(data['input'][i])\n",
        "\n",
        "    chars = extract_kr_char(data['input'][i])\n",
        "\n",
        "    if train_flg:\n",
        "      orgs = extract_kr_char(data['output'][i])\n",
        "      chos_, jungs_, jongs_, _ = preprocessing(data['output'][i])\n",
        "\n",
        "      temp = pd.DataFrame({'index':i, '원본':orgs,'난독화':chars, '위치': poss,\n",
        "                        '초성':chos_, '중성': jungs_, '종성':jongs_,\n",
        "                        '난독화_초성':chos, '난독화_중성': jungs, '난독화_종성':jongs})\n",
        "    else:\n",
        "      temp = pd.DataFrame({'index':i,'난독화':chars, '위치': poss,\n",
        "                        '난독화_초성':chos, '난독화_중성': jungs, '난독화_종성':jongs})\n",
        "    df_prep = pd.concat([df_prep,temp])\n",
        "    df_prep.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  if train_flg:\n",
        "    n = 4\n",
        "  else:\n",
        "    n = 3\n",
        "\n",
        "  data = df_prep.iloc[:,n:].values\n",
        "  encoded_data = Encoder(data, jamo_to_index)\n",
        "  encoded_data = np.array(encoded_data)\n",
        "\n",
        "  df_encoded = pd.DataFrame(encoded_data,\n",
        "                             columns=[f'{col}_ENC'for col in df_prep.columns[n:]])\n",
        "  df_prep = pd.concat([df_prep, df_encoded], axis=1)\n",
        "\n",
        "  return df_prep"
      ],
      "metadata": {
        "id": "3nhz59ogTwR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_prep = preprocessing_encoding(TRAIN)\n",
        "TEST_prep = preprocessing_encoding(TEST, train_flg = False)"
      ],
      "metadata": {
        "id": "qkezWzigYS4z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_prep.to_csv(path+'/TRAIN_prep.csv', index=False)\n",
        "TEST_prep.to_csv(path+'/TEST_prep.csv', index=False)"
      ],
      "metadata": {
        "id": "SJ9CCcVIYhag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 훈련을 위한 train, valid split"
      ],
      "metadata": {
        "id": "RAw5l5nNbmyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = pd.read_csv(path+\"/train.csv\")\n",
        "TEST = pd.read_csv(path+\"/test.csv\")"
      ],
      "metadata": {
        "id": "AAyueQJtg8Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = TRAIN.iloc[:,:-1], TRAIN.iloc[:,-1]\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "train_model = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
        "valid_model = pd.concat([X_valid, y_valid], axis=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ISPISj_Db5NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model.to_csv(path+'/train_model.csv', index=False)\n",
        "valid_model.to_csv(path+'/valid_model.csv', index=False)"
      ],
      "metadata": {
        "id": "hsRZNCQMcA0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전처리"
      ],
      "metadata": {
        "id": "iNH_mo78dQf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_prep = preprocessing_encoding(train_model)\n",
        "valid_prep = preprocessing_encoding(valid_model)"
      ],
      "metadata": {
        "id": "VuNZQ4jYdQIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prep.to_csv(path+'/train_prep.csv', index=False)\n",
        "valid_prep.to_csv(path+'/valid_prep.csv', index=False)"
      ],
      "metadata": {
        "id": "9CjLPfQQUiID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1차 해독 (MLP)"
      ],
      "metadata": {
        "id": "XwIJ3r_kYiqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "3YfoQgiCYkdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import torch.optim as optim\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rc('font', family='NanumGothic')\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "-P4XqU5hZY1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP training\n",
        "- AdamW + LR Scheduler"
      ],
      "metadata": {
        "id": "9Wwr9PuWcTqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_prep = pd.read_csv(path+'/train_prep.csv')\n",
        "valid_prep = pd.read_csv(path+'/valid_prep.csv')"
      ],
      "metadata": {
        "id": "NW4eTU5LJJ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_prep.iloc[:,13:].values, train_prep.iloc[:,10:13].values\n",
        "X_valid, y_valid = valid_prep.iloc[:,13:].values, valid_prep.iloc[:,10:13].values\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.long)\n",
        "y_valid = torch.tensor(y_valid, dtype=torch.long)"
      ],
      "metadata": {
        "id": "N41UnuiNcImK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train = X_train\n",
        "X2_train = torch.tensor(train_prep.iloc[:,3].values, dtype = torch.float)\n",
        "X1_valid = X_valid\n",
        "X2_valid = torch.tensor(valid_prep.iloc[:,3].values, dtype = torch.float)"
      ],
      "metadata": {
        "id": "6x_Wfdz3d8aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self,embedding_dim, num_jamo=68):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.jamo_embedding = nn.Embedding(num_jamo, embedding_dim//3)\n",
        "        self.pos_embedding = nn.Sequential(\n",
        "            nn.Linear(1, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, embedding_dim%3)\n",
        "            )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.jamo_embedding(x1)\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "\n",
        "        x2 = x2.unsqueeze(-1)\n",
        "        x2 = self.pos_embedding(x2)\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=-1)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "    self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.fc2(x) + x # skip connection\n",
        "    x = self.bn2(x)\n",
        "    x = torch.tanh(x)\n",
        "    return x\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hidden_dim = 128, embedding_dim = 64):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embedding(embedding_dim)\n",
        "    self.mlp = MLP(embedding_dim, hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, 68)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x = self.embedding(x1,x2)\n",
        "    x = self.mlp(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)  # Python의 random 모듈 시드 고정\n",
        "    np.random.seed(seed)  # NumPy 시드 고정\n",
        "    torch.manual_seed(seed)  # PyTorch의 시드 고정\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 시드 고정 (GPU 사용 시)\n",
        "    torch.cuda.manual_seed_all(seed)  # 다중 GPU 환경 시드 고정\n",
        "    torch.backends.cudnn.deterministic = True  # CUDNN 연산을 결정적으로 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 최적화 비활성화 (재현성 보장)\n",
        "\n",
        "set_seed(42)  # 원하는 시드 값 설정\n",
        "\n",
        "class CustomDataset(TensorDataset):\n",
        "    def __init__(self, jamo, position, labels):\n",
        "\n",
        "        self.jamo = jamo\n",
        "        self.position = position\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        jamo_sample = self.jamo[idx]\n",
        "        pos_sample = self.position[idx]\n",
        "        label = self.labels[idx]\n",
        "        return jamo_sample, pos_sample, label\n"
      ],
      "metadata": {
        "id": "5DAueOD-eBaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련 / 검증 데이터 분할 학습\n",
        "인사이트 도출용이므로 주석처리하고 넘어가도 됨"
      ],
      "metadata": {
        "id": "z-OmPbIWMRyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vT72KPDzGsb"
      },
      "outputs": [],
      "source": [
        "# from torch.optim import AdamW\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = Decoder(hidden_dim = 256, embedding_dim = 128).to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train_dataset = CustomDataset(X1_train, X2_train, y_train)\n",
        "# valid_dataset = CustomDataset(X1_valid, X2_valid, y_valid)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ff7N1ApzGsb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "# # 10 epoch 동안 val_loss가 감소하지 않으면, 학습률 0.5 감소\n",
        "# model.train()\n",
        "\n",
        "# epochs = 200\n",
        "# loss_curve = np.zeros((2,epochs))\n",
        "# best_val_loss = float('inf')\n",
        "# save_path = path + \"/best_model.pth\"\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     total_loss = 0\n",
        "#     # 학습\n",
        "#     for batch_idx, (jamo, pos, target) in enumerate(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "#         jamo, pos, target = jamo.to(device), pos.to(device), target.to(device)\n",
        "#         logits = model(jamo, pos)\n",
        "\n",
        "#         logit_cho = logits[:, :19]\n",
        "#         logit_jung = logits[:, 19:40]\n",
        "#         logit_jong = logits[:, 40:]\n",
        "\n",
        "#         loss1 = criterion(logit_cho, target[:, 0])\n",
        "#         loss2 = criterion(logit_jung, target[:, 1] - 19)\n",
        "#         loss3 = criterion(logit_jong, target[:, 2] - 40)\n",
        "#         loss = loss1 + loss2 + loss3\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         if batch_idx % 4500 == 0:\n",
        "#             print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "#     print(f'====> Epoch: {epoch}, Total Loss: {total_loss / (len(X_train) // 64):.4f}')\n",
        "#     loss_curve[0,epoch] = total_loss / (len(X_train) // 64)\n",
        "\n",
        "#     # 검증\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for jamo, pos, target in valid_loader:\n",
        "#             jamo, pos, target = jamo.to(device), pos.to(device), target.to(device)\n",
        "#             logits = model(jamo, pos)\n",
        "\n",
        "#             logit_cho = logits[:, :19]\n",
        "#             logit_jung = logits[:, 19:40]\n",
        "#             logit_jong = logits[:, 40:]\n",
        "\n",
        "#             loss1 = criterion(logit_cho, target[:, 0])\n",
        "#             loss2 = criterion(logit_jung, target[:, 1] - 19)\n",
        "#             loss3 = criterion(logit_jong, target[:, 2] - 40)\n",
        "#             loss = loss1 + loss2 + loss3\n",
        "\n",
        "#             val_loss += loss.item()\n",
        "\n",
        "#     val_loss /= len(valid_loader)\n",
        "#     print(f'====> Validation Loss: {val_loss:.4f}')\n",
        "#     loss_curve[1,epoch] = val_loss\n",
        "#     scheduler.step(val_loss) # 스케줄러 업데이트\n",
        "\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'val_loss': best_val_loss,\n",
        "#         }, save_path)\n",
        "#         print(f\"Model Saved. (epoch: {epoch}, val_loss: {val_loss:.4f})\")\n",
        "#     model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWwCziIIzGsb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# plt.plot(loss_curve[0], label='train')\n",
        "# plt.plot(loss_curve[1], label='valid')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련 데이터 전체 학습"
      ],
      "metadata": {
        "id": "gYOgIMOuNQ3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Decoder(hidden_dim = 256, embedding_dim = 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X1_train = torch.vstack([X1_train, X1_valid])\n",
        "X2_train = torch.hstack([X2_train, X2_valid])\n",
        "y_train = torch.vstack([y_train, y_valid])\n",
        "\n",
        "train_dataset = CustomDataset(X1_train, X2_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "zGcW3-_2fFU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "scheduler = MultiStepLR(optimizer, milestones = range(45,165,15), gamma=0.5)\n",
        "model.train()\n",
        "\n",
        "epochs = 200\n",
        "loss_curve = np.zeros((1,epochs))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    # 학습\n",
        "    for batch_idx, (jamo, pos, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        jamo, pos, target = jamo.to(device), pos.to(device), target.to(device)\n",
        "        logits = model(jamo, pos)\n",
        "\n",
        "        logit_cho = logits[:, :19]\n",
        "        logit_jung = logits[:, 19:40]\n",
        "        logit_jong = logits[:, 40:]\n",
        "\n",
        "        loss1 = criterion(logit_cho, target[:, 0])\n",
        "        loss2 = criterion(logit_jung, target[:, 1] - 19)\n",
        "        loss3 = criterion(logit_jong, target[:, 2] - 40)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 4500 == 0:\n",
        "          lr = optimizer.param_groups[0]['lr']\n",
        "          print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}, LR: {lr}')\n",
        "\n",
        "    print(f'====> Epoch: {epoch}, Total Loss: {total_loss / (len(X1_train) // 64):.4f}')\n",
        "    loss_curve[0,epoch] = total_loss / (len(X_train) // 64)\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch + 1 in range(50,201,10):\n",
        "      save_path = f\"{path}/full_model_{epoch+1}.pth\"\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict()\n",
        "        }, save_path)\n",
        "      print(f\"Model Saved. (epoch: {epoch+1})\")"
      ],
      "metadata": {
        "id": "ITUC_9qSfPlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_curve[0], label='train')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CNloU2HLfj_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해독"
      ],
      "metadata": {
        "id": "DHkxTpD4geFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = pd.read_csv(path+'/train.csv') #원본\n",
        "TRAIN_prep = pd.read_csv(path+'/TRAIN_prep.csv') #원본\n",
        "TEST = pd.read_csv(path+'/test.csv')\n",
        "TEST_prep = pd.read_csv(path+'/TEST_prep.csv')\n",
        "\n",
        "train = pd.read_csv(path+'/train_model.csv')\n",
        "train_prep = pd.read_csv(path+'/train_prep.csv')\n",
        "valid = pd.read_csv(path+'/valid_model.csv')\n",
        "valid_prep = pd.read_csv(path+'/valid_prep.csv')"
      ],
      "metadata": {
        "id": "5ksKzlS3ge5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cho = [chr(i) for i in range(0x1100, 0x1113)]  # 초성 (ㄱ~ㅎ)\n",
        "jung = [chr(i) for i in range(0x1161, 0x1176)]  # 중성 (ㅏ~ㅣ)\n",
        "jong = [chr(i) for i in range(0x11A8, 0x11C3)]  # 종성 (ㄱ~ㅄ)\n",
        "jamo = cho + jung + jong + ['ㅉ']\n",
        "\n",
        "jamo_to_index = {jamo[i]:i for i in range(len(jamo))}\n",
        "index_to_jamo = {i:jamo[i] for i in range(len(jamo))}"
      ],
      "metadata": {
        "id": "EU5jNlj5PlFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_jamos(choseong_str, jungseong_str, jongseong_str='ㅉ'):\n",
        "\n",
        "    # 초성, 중성 인덱스 계산\n",
        "    choseong_index = cho.index(choseong_str)\n",
        "    jungseong_index = jung.index(jungseong_str)\n",
        "\n",
        "    # 종성 인덱스 계산 (받침 없는 경우 0으로 처리)\n",
        "    if jongseong_str == 'ㅉ':  # 받침이 없는 경우\n",
        "        jongseong_index = 0\n",
        "    else:\n",
        "        jongseong_index = jong.index(jongseong_str) + 1  # 종성은 1부터 시작\n",
        "\n",
        "    # 유니코드 한글 음절 계산\n",
        "    HANGUL_BASE = 0xAC00\n",
        "    syllable_code = (\n",
        "        HANGUL_BASE\n",
        "        + choseong_index * 21 * 28  # 초성\n",
        "        + jungseong_index * 28      # 중성\n",
        "        + jongseong_index           # 종성\n",
        "    )\n",
        "\n",
        "    return chr(syllable_code)\n"
      ],
      "metadata": {
        "id": "S8NWVZahQVm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, data, data_prep, index_to_jamo = index_to_jamo):\n",
        "\n",
        "  X1 = data_prep[['난독화_초성_ENC','난독화_중성_ENC','난독화_종성_ENC']].values\n",
        "  X2 = data_prep['위치'].values\n",
        "  X1, X2 = torch.tensor(X1, dtype = torch.long), torch.tensor(X2, dtype = torch.float)\n",
        "\n",
        "  model.eval()\n",
        "  model.to('cpu')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(X1, X2)\n",
        "    logit_cho = logits[:,:19]\n",
        "    logit_jung = logits[:,19:40]\n",
        "    logit_jong = logits[:,40:]\n",
        "\n",
        "  pred_cho = torch.argmax(logit_cho, dim=1)\n",
        "  pred_jung = torch.argmax(logit_jung, dim=1) + 19\n",
        "  pred_jong = torch.argmax(logit_jong, dim=1) + 40\n",
        "  pred_jamo = torch.stack([pred_cho, pred_jung, pred_jong], dim=1)\n",
        "\n",
        "  pred = pd.DataFrame(pred_jamo, columns=['초성', '중성', '종성'])\n",
        "  pred['결과'] = pred.apply(lambda x : combine_jamos(index_to_jamo[x['초성']],\n",
        "                                                   index_to_jamo[x['중성']],\n",
        "                                                   index_to_jamo[x['종성']]), axis =1)\n",
        "\n",
        "\n",
        "  chos = [chr(i) for i in range(0x1100, 0x1113)] # 초성 유니코드\n",
        "  jungs = [chr(i) for i in range(0x1161, 0x1176)] # 중성 유니코드\n",
        "  jongs = [chr(i) for i in range(0x11A8, 0x11C3)] # 종성 유니코드\n",
        "  jamo = chos + jungs + jongs + ['ㅉ']\n",
        "\n",
        "  jamo_to_index = {jamo[i]:i for i in range(len(jamo))}\n",
        "  index_to_jamo = {i:jamo[i] for i in range(len(jamo))}\n",
        "\n",
        "  decoded_sentences = []\n",
        "  i = 0\n",
        "\n",
        "  for _, sentence in data.iterrows():\n",
        "    decoded_sentence = ''\n",
        "    for char in sentence['input']:\n",
        "      if 0xAC00 <= ord(char) <= 0xD7A3:\n",
        "        decoded_sentence += pred.loc[i, '결과']\n",
        "        i += 1\n",
        "\n",
        "      else:\n",
        "        decoded_sentence += char\n",
        "    decoded_sentences.append(decoded_sentence)\n",
        "\n",
        "  return decoded_sentences, pred_jamo"
      ],
      "metadata": {
        "id": "tBmIvrnliNXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = path+\"/full_model_140.pth\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint = torch.load(save_path)\n",
        "model_ = Decoder(embedding_dim = 128, hidden_dim = 256).to(device)\n",
        "model_.load_state_dict(checkpoint['model_state_dict'])\n",
        "model_.to('cpu')\n",
        "\n",
        "def compute_accuracy(data, pred):\n",
        "  acc1 = (data.iloc[:,10:13].values == pred).all(axis=1).sum()/len(data)\n",
        "  acc2 = (data.iloc[:,10:13].values == pred).reshape(-1,).sum()/len(data)/3\n",
        "  return acc1, acc2"
      ],
      "metadata": {
        "id": "FaPFDCKUiWvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process_and_save_data(model, data, data_prep, dataset_type, path = path):\n",
        "    result, pred = inference(model, data, data_prep)\n",
        "    if dataset_type in ['train','TRAIN', 'valid']:\n",
        "      dec1_df = pd.DataFrame({\n",
        "        'input': data['input'],\n",
        "        'input_DEC1': result,\n",
        "        'output': data['output']\n",
        "        })\n",
        "    else:\n",
        "      dec1_df = pd.DataFrame({\n",
        "        'input': data['input'],\n",
        "        'input_DEC1': result\n",
        "        })\n",
        "\n",
        "    dec1_df.to_csv(path + f\"/{dataset_type}_dec1.csv\", index=False)\n",
        "    return dec1_df\n",
        "\n",
        "process_and_save_data(model_, TRAIN, TRAIN_prep, 'TRAIN')\n",
        "process_and_save_data(model_, train, train_prep, 'train')\n",
        "process_and_save_data(model_, valid, valid_prep, 'valid')\n",
        "process_and_save_data(model_, TEST, TEST_prep, 'TEST')"
      ],
      "metadata": {
        "id": "bguj4aHfjLJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차 해독 _electra"
      ],
      "metadata": {
        "id": "FFGvq2baj3cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(path+\"/TRAIN_dec1.csv\")\n",
        "test = pd.read_csv(path+\"/TEST_dec1.csv\")"
      ],
      "metadata": {
        "id": "FZUSuEZXj6_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking"
      ],
      "metadata": {
        "id": "YXaY2mqWz-6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. 한국어(가-힣), 영어(a-zA-Z), 숫자(0-9), 공백(\\s) 외 특수문자 제거\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "\n",
        "    # 2. ㅋㅋ, ㅜㅜ, ㅠㅠ, ㅎㅎ 등과 같은 불완전한 한글 제거\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    # 온점, 느낌표, 물음표 뒤에 공백이 있으면 모두 분리\n",
        "    sentences = re.split(r'(?<=[.!?])\\s', text)\n",
        "    return [sentence for sentence in sentences if sentence]\n",
        "\n",
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text"
      ],
      "metadata": {
        "id": "WtbizK8Ty-j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['cleaned_input'] = train['input_DEC1'].apply(clean_text)\n",
        "test['cleaned_input'] = test['input_DEC1'].apply(clean_text)\n",
        "train['cleaned_output'] = train['output'].apply(clean_text)"
      ],
      "metadata": {
        "id": "sYtpP-lld8mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['chunked_input'] = train['cleaned_input'].apply(split_sentences)\n",
        "test['chunked_input'] = test['cleaned_input'].apply(split_sentences)\n",
        "train['chunked_output'] = train['cleaned_output'].apply(split_sentences)\n",
        "\n",
        "train_exploded = train.explode('chunked_input')[['chunked_input']].reset_index(drop=False)\n",
        "train_exploded_ = train.explode('chunked_output')[['chunked_output']].reset_index(drop=True)\n",
        "\n",
        "test_chunked = test.explode('chunked_input')[['chunked_input']].reset_index(drop=False)\n",
        "train_chunked = pd.concat([train_exploded, train_exploded_], axis=1)"
      ],
      "metadata": {
        "id": "Zm9DmgZg7uqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked['chunked_input'].apply(len).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "gAQ2j1UC8K1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked.loc[2435]"
      ],
      "metadata": {
        "id": "GRPYbb5a8ZhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked[train_chunked['index'] == 658]"
      ],
      "metadata": {
        "id": "wuI20zkC86KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 청킹 후에도 긴 문장에 대해 수동 분할\n",
        "input1, input2 = train_chunked.iloc[2435][1][:423], train_chunked.iloc[2435][1][424:]\n",
        "input3 = train_chunked.iloc[2436][1]\n",
        "output1, output2 = train_chunked.iloc[2435][2][:423], train_chunked.iloc[2435][2][424:]\n",
        "output3 = train_chunked.iloc[2436][2]\n",
        "temp = pd.DataFrame({'index':[658,658,658], 'chunked_input':[input1,input2,input3],\n",
        "                     'chunked_output':[output1,output2,output3]})\n",
        "train_chunked.drop(index=[2435,2436], inplace=True)\n",
        "train_chunked = pd.concat([train_chunked, temp]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "JC2CJMoA8sro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked"
      ],
      "metadata": {
        "id": "_k9Y_BLlBeR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked['chunked_input'] = train_chunked.apply(lambda x: clean_blank(x['chunked_input']), axis = 1)\n",
        "train_chunked['chunked_output'] = train_chunked.apply(lambda x: clean_blank(x['chunked_output']), axis = 1)\n",
        "train_chunked['len'] = train_chunked['chunked_input'].apply(len)\n",
        "train_chunked.drop(train_chunked[train_chunked['len'] == 0].index, inplace=True)\n",
        "train_chunked.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_chunked['chunked_input'] = test_chunked.apply(lambda x: clean_blank(x['chunked_input']), axis = 1)\n",
        "test_chunked['len'] = test_chunked['chunked_input'].apply(len)\n",
        "test_chunked.drop(test_chunked[test_chunked['len'] == 0].index, inplace=True)\n",
        "test_chunked.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "IbgD7kCW0OdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked.drop(columns=['len'], inplace=True)\n",
        "test_chunked.drop(columns=['len'], inplace=True)"
      ],
      "metadata": {
        "id": "BHtcowES9tD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked.to_csv(path+\"/TRAIN_dec1_chunked.csv\", index=False)\n",
        "test_chunked.to_csv(path+\"/TEST_dec1_chunked.csv\", index=False)"
      ],
      "metadata": {
        "id": "dYxd4O470oX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_chunked = pd.read_csv(path+\"/TRAIN_dec1_chunked.csv\")\n",
        "test_chunked = pd.read_csv(path+\"/TEST_dec1_chunked.csv\")"
      ],
      "metadata": {
        "id": "Eb8kLYbpVPS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "cLzKxkSI3KQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "JdI9b63Z4zJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_original_text(original_text, decoded_text):\n",
        "\n",
        "    restored_text = \"\"\n",
        "    decoded_idx = 0\n",
        "    # 한글, 영어, 숫자, 공백, 문장부호만 허용하는 정규식\n",
        "    valid_char_pattern = re.compile(r'[가-힣a-zA-Z0-9(),.!?\\'\\\"\\[\\]\\{\\}:]')\n",
        "\n",
        "\n",
        "    for char in original_text:\n",
        "        if valid_char_pattern.match(char):\n",
        "            # 유효한 문자일 경우, 해독된 문자로 대체\n",
        "            if decoded_idx < len(decoded_text):\n",
        "                restored_text += decoded_text[decoded_idx]\n",
        "                decoded_idx += 1\n",
        "            else:\n",
        "                # 만약 해독된 텍스트가 끝났다면 원문 문자 그대로 추가\n",
        "                restored_text += char\n",
        "        else:\n",
        "            # 특수문자 및 불완전한 한글은 원문 그대로 유지\n",
        "            restored_text += char\n",
        "\n",
        "    return restored_text"
      ],
      "metadata": {
        "id": "WCFer7JG3OYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV 파일을 JSON 파일로 변환하는 함수\n",
        "def csv_to_json(csv_file_path, json_file_path):\n",
        "    with open(csv_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        data = list(reader)\n",
        "\n",
        "    # JSON 파일 저장\n",
        "    with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "csv_file_path = path+\"/TRAIN_dec1_chunked.csv\"\n",
        "json_file_path = path+\"/TRAIN_dataset.json\"  # 변환할 JSON 파일 경로\n",
        "\n",
        "csv_to_json(csv_file_path, json_file_path)\n",
        "print(f\"변환 완료: {json_file_path}\")"
      ],
      "metadata": {
        "id": "cfIjTHjK3YPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "# SentencePiece 모델 학습 (음절 단위)\n",
        "os.chdir(path)\n",
        "spm.SentencePieceTrainer.Train(f'--input=TRAIN_dataset.json --model_prefix=syllable --vocab_size=12000 --character_coverage=1.0 --model_type=char')\n",
        "\n",
        "# SentencePiece 모델 로드\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"syllable.model\")"
      ],
      "metadata": {
        "id": "ImDeHJey3bdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Gj4ZL6-t38dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train = pd.read_csv(path+\"/TRAIN_dec1.csv\")\n",
        "test = pd.read_csv(path+\"/TEST_dec1.csv\")\n",
        "\n",
        "train_chunked = pd.read_csv(path+\"/TRAIN_dec1_chunked.csv\")\n",
        "test_chunked = pd.read_csv(path+\"/TEST_dec1_chunked.csv\")"
      ],
      "metadata": {
        "id": "Q5tjaWis39oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
        "import sentencepiece as spm\n",
        "\n",
        "# 기존 KoELECTRA 모델의 토크나이저 로드\n",
        "base_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "\n",
        "tokenizer_path = path+\"/syllable.model\"\n",
        "vocab_path = path+\"/syllable.vocab\"\n",
        "\n",
        "# SentencePiece를 이용한 커스텀 토크나이저 적용\n",
        "class CustomTokenizer(ElectraTokenizer):\n",
        "    def __init__(self, sp_model_path, *args, **kwargs):\n",
        "        super().__init__(vocab_file=vocab_path, *args, **kwargs)\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(sp_model_path)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.sp_model.EncodeAsPieces(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.sp_model.PieceToId(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return [self.sp_model.IdToPiece(id) for id in ids]\n",
        "\n",
        "# Custom Tokenizer 생성\n",
        "custom_tokenizer = CustomTokenizer(tokenizer_path)\n",
        "\n",
        "# KoELECTRA 모델 로드\n",
        "model = ElectraForMaskedLM.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
      ],
      "metadata": {
        "id": "lnpMmBwa4c7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_original_text(original_text, decoded_text):\n",
        "\n",
        "    restored_text = \"\"\n",
        "    decoded_idx = 0  # 해독된 텍스트에서 현재 참조 중인 문자 위치\n",
        "\n",
        "    # 한글, 영어, 숫자, 공백, 문장부호만 허용하는 정규식\n",
        "    valid_char_pattern = re.compile(r'[가-힣a-zA-Z0-9(),.!?\\'\\\"\\[\\]\\{\\}:]')\n",
        "\n",
        "\n",
        "    for char in original_text:\n",
        "        if valid_char_pattern.match(char):\n",
        "            # 유효한 문자일 경우, 해독된 문자로 대체\n",
        "            if decoded_idx < len(decoded_text):\n",
        "                restored_text += decoded_text[decoded_idx]\n",
        "                decoded_idx += 1\n",
        "            else:\n",
        "                # 만약 해독된 텍스트가 끝났다면 원문 문자 그대로 추가\n",
        "                restored_text += char\n",
        "        else:\n",
        "            # 특수문자 및 불완전한 한글은 원문 그대로 유지\n",
        "            restored_text += char\n",
        "\n",
        "    return restored_text\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. 한국어(가-힣), 영어(a-zA-Z), 숫자(0-9), 공백(\\s) 외 특수문자 제거\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "\n",
        "    # 2. ㅋㅋ, ㅜㅜ, ㅠㅠ, ㅎㅎ 등과 같은 불완전한 한글 제거\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text"
      ],
      "metadata": {
        "id": "SBw_jOVW4esB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataloader"
      ],
      "metadata": {
        "id": "8fQWAyyY4qsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class ObfuscatedDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length  # 최대 길이 지정\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_tokens = self.tokenizer.tokenize(item[\"chunked_input\"])\n",
        "        target_tokens = self.tokenizer.tokenize(item[\"chunked_output\"])\n",
        "\n",
        "        # 토큰을 ID로 변환\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "        target_ids = self.tokenizer.convert_tokens_to_ids(target_tokens)\n",
        "\n",
        "        # **고정된 max_length로 패딩**\n",
        "        input_ids = input_ids[:self.max_length]\n",
        "        target_ids = target_ids[:self.max_length]\n",
        "\n",
        "        input_ids += [0] * (self.max_length - len(input_ids))  # 패딩 추가\n",
        "        target_ids += [0] * (self.max_length - len(target_ids))\n",
        "\n",
        "        # **어텐션 마스크 생성 (패딩된 부분: 0, 나머지: 1)**\n",
        "        attention_mask = [1 if token != 0 else 0 for token in target_ids]\n",
        "\n",
        "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long)\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_dataset = ObfuscatedDataset(path+\"/TRAIN_dataset.json\", custom_tokenizer, max_length=512)\n",
        "\n",
        "# DataLoader 설정\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "MEUBI7sW4qJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- loss & Optimizer"
      ],
      "metadata": {
        "id": "C_rjVLuk49KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "num_epochs = 20\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "loss_curve = np.zeros((1,num_epochs))"
      ],
      "metadata": {
        "id": "XRqXy31e4-9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training"
      ],
      "metadata": {
        "id": "ISu_doI45A_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)  # Python의 random 모듈 시드 고정\n",
        "    np.random.seed(seed)  # NumPy 시드 고정\n",
        "    torch.manual_seed(seed)  # PyTorch의 시드 고정\n",
        "    torch.cuda.manual_seed(seed)  # CUDA 시드 고정 (GPU 사용 시)\n",
        "    torch.cuda.manual_seed_all(seed)  # 다중 GPU 환경 시드 고정\n",
        "    torch.backends.cudnn.deterministic = True  # CUDNN 연산을 결정적으로 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 연산 최적화 비활성화 (재현성 보장)\n",
        "\n",
        "set_seed(42)  # 원하는 시드 값 설정"
      ],
      "metadata": {
        "id": "OBE8sZYd5CB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    save_path = path+f\"/full_electra_epc{epoch+1}.pth\"\n",
        "\n",
        "    for batch_idx, (input_ids, target_ids, attention_mask) in enumerate(train_dataloader):\n",
        "        input_ids, target_ids, attention_mask = (\n",
        "            input_ids.to(device),\n",
        "            target_ids.to(device),\n",
        "            attention_mask.to(device),\n",
        "        )\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask).logits\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # 100 배치마다 진행 상황 출력\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = total_train_loss / (batch_idx + 1)\n",
        "            print(f\"[Train] Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    loss_curve[0,epoch] = avg_train_loss\n",
        "    print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    if epoch >= 5:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict()\n",
        "        }, save_path)\n",
        "        print(f\"Model Saved. (epoch: {epoch+1})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pU45JGiz5KYb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_curve[0][1:], label='train')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lPvMp9sm5Lud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2차 해독"
      ],
      "metadata": {
        "id": "wcop_qZH5QA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(path+\"/TRAIN_dec1.csv\")\n",
        "test = pd.read_csv(path+\"/TEST_dec1.csv\")\n",
        "train_chunked = pd.read_csv(path+\"/TRAIN_dec1_chunked.csv\")\n",
        "test_chunked = pd.read_csv(path+\"/TEST_dec1_chunked.csv\")"
      ],
      "metadata": {
        "id": "nhTpFl9v5Ril"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ElectraForMaskedLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ElectraForMaskedLM.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "checkpoint = torch.load(path+\"/full_electra_epc20.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def denoise_text(input_text, tokenizer, max_length=512):\n",
        "\n",
        "    input_tokens = tokenizer.tokenize(input_text)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "\n",
        "    input_ids = input_ids[:max_length]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    input_ids += [0] * (max_length - len(input_ids))  # 패딩\n",
        "    attention_mask += [0] * (max_length - len(attention_mask))\n",
        "\n",
        "    input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "    attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids_tensor, attention_mask=attention_mask_tensor).logits\n",
        "        predicted_ids = torch.argmax(outputs, dim=-1)[0].cpu().tolist()\n",
        "\n",
        "    final_ids = [\n",
        "        pred if a == 0 else (pred if pred != 0 else 59)\n",
        "        for orig, pred, a in zip(input_ids, predicted_ids, attention_mask)\n",
        "    ]\n",
        "\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(final_ids)\n",
        "    restored_text = \"\".join(predicted_tokens).replace(\"▁\", \" \")\n",
        "\n",
        "    return restored_text\n",
        "\n",
        "def check_double_blanks(text):\n",
        "    return bool(re.search(r' {2,}', text))\n",
        "\n",
        "def check_repeated_last_char(text):\n",
        "    return bool(re.search(r'([가-힣])\\1+$', text))\n",
        "\n",
        "\n",
        "def remove_trailing_unks(text):\n",
        "\n",
        "    cleaned_text = re.sub(r'(<unk>\\s*)+$', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def post_processing(data, data_chunked, result, test_flg = True):\n",
        "  temp = data.copy()\n",
        "  temp_chunked = data_chunked.copy()\n",
        "\n",
        "  for i in range(len(result)):\n",
        "    result[i] = remove_trailing_unks(result[i])\n",
        "\n",
        "  temp_chunked['input_DEC2'] = result\n",
        "  data_recovered = temp_chunked.groupby(\"index\").agg(list).reset_index(drop=True)\n",
        "  if test_flg:\n",
        "    data_recovered.columns = ['input_DEC1','input_DEC2']\n",
        "    data_recovered['input_DEC2'] = data_recovered['input_DEC2'].apply(lambda x: ' '.join(x))\n",
        "  else:\n",
        "    data_recovered.columns = ['input_DEC1','output','input_DEC2']\n",
        "    data_recovered['input_DEC2'] = data_recovered['input_DEC2'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "  temp['input_DEC1'] = temp.apply(lambda x: clean_blank(x['input_DEC1']), axis = 1)\n",
        "\n",
        "  temp['input_DEC2'] = data_recovered['input_DEC2']\n",
        "  temp['input_DEC2'] = temp.apply(lambda x: clean_blank(x['input_DEC2']), axis = 1)\n",
        "  temp['input_DEC2'] = temp.apply(lambda x: x['input_DEC2'].replace(\" \",\"\"), axis= 1)\n",
        "\n",
        "  temp['input_DEC2'] = temp.apply(lambda x: restore_original_text(x['input_DEC1'],x['input_DEC2']), axis = 1)\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "ipjU79EX5S9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "test_sentences = test_chunked['chunked_input']\n",
        "\n",
        "decoded = []\n",
        "for sentence in tqdm(test_sentences):\n",
        "    restored = denoise_text(sentence, custom_tokenizer)\n",
        "    decoded.append(restored)\n",
        "\n",
        "test_result = post_processing(test, test_chunked, decoded)"
      ],
      "metadata": {
        "id": "fR4NF06wufDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result.to_csv(path+\"/TEST_dec2.csv\", index=False)"
      ],
      "metadata": {
        "id": "X19TzSKH766w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
        "submission['output'] = test_result['input_DEC2']\n",
        "submission.to_csv(path+\"/submission_electra.csv\", index=False)"
      ],
      "metadata": {
        "id": "fNaiNd92NG0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차 해독_후처리"
      ],
      "metadata": {
        "id": "e1lbWz1r8WDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# 잘 학습된 ELECTRA로 추론된 test dec2 셋\n",
        "test = pd.read_csv(path+\"/TEST_dec2.csv\")"
      ],
      "metadata": {
        "id": "80ty7gW38XLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # 1. 한국어(가-힣), 영어(a-zA-Z), 숫자(0-9), 공백(\\s) 외 특수문자 제거\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "\n",
        "    # 2. ㅋㅋ, ㅜㅜ, ㅠㅠ, ㅎㅎ 등과 같은 불완전한 한글 제거\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    # 기본적인 문장 분리\n",
        "    sentences = re.split(r'(?<=[.!?])\\s', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence]  # 공백 제거 및 빈 요소 제거\n",
        "\n",
        "    merged_sentences = []\n",
        "    temp = \"\"\n",
        "    open_bracket_count = 0  # 열린 괄호 개수\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # 괄호 개수 체크\n",
        "        open_bracket_count += sentence.count(\"(\")\n",
        "        open_bracket_count -= sentence.count(\")\")\n",
        "\n",
        "        # 문장이 20자 이하이거나, 괄호가 닫히지 않았다면 다음 문장과 결합\n",
        "        if (len(sentence) <= 20 and i < len(sentences) - 1) or open_bracket_count > 0:\n",
        "            temp += sentence + \" \"\n",
        "        else:\n",
        "            temp += sentence\n",
        "            merged_sentences.append(temp)\n",
        "            temp = \"\"  # temp 초기화\n",
        "\n",
        "    if temp:  # 마지막 문장이 남아 있다면 추가\n",
        "        merged_sentences.append(temp.strip())\n",
        "\n",
        "    # 마지막 문장은 반드시 마지막에서 두 번째 문장과 결합\n",
        "    if len(merged_sentences) > 1:\n",
        "        merged_sentences[-2] += \" \" + merged_sentences[-1]\n",
        "        merged_sentences.pop()  # 마지막 문장 제거 (이미 합쳐졌으므로)\n",
        "\n",
        "    return merged_sentences\n",
        "\n",
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text"
      ],
      "metadata": {
        "id": "gFjiRPrO8aFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['cleaned_input_DEC2'] = test['input_DEC2'].apply(clean_text)\n",
        "\n",
        "test_ = test.copy()\n",
        "test_['chunked_input'] = test['cleaned_input_DEC2'].apply(split_sentences)\n",
        "test_chunked = test_.explode('chunked_input')[['chunked_input']].reset_index(drop=False)\n",
        "\n",
        "test_chunked['chunked_input'] = test_chunked['chunked_input'].apply(clean_blank)\n",
        "\n",
        "test_chunked.drop(test_chunked[test_chunked['chunked_input'] == ''].index, inplace=True)\n",
        "test_chunked.reset_index(drop=True, inplace=True)\n",
        "test_chunked.to_csv(path+\"/TEST_dec2_chunked.csv\", index=False)"
      ],
      "metadata": {
        "id": "RKrRIzac8fKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# koGemma-7b finetuning을 위한 전처리"
      ],
      "metadata": {
        "id": "qKgLWMvaXLax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    text = str(text)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s', text)\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    merged = []\n",
        "    temp = \"\"\n",
        "    open_bracket_count = 0\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        open_bracket_count += sentence.count(\"(\")\n",
        "        open_bracket_count -= sentence.count(\")\")\n",
        "        if (len(sentence) <= 20 and i < len(sentences) - 1) or open_bracket_count > 0:\n",
        "            temp += sentence + \" \"\n",
        "        else:\n",
        "            temp += sentence\n",
        "            merged.append(temp.strip())\n",
        "            temp = \"\"\n",
        "    if temp:\n",
        "        merged.append(temp.strip())\n",
        "    if len(merged) > 1:\n",
        "        merged[-2] = merged[-2] + \" \" + merged[-1]\n",
        "        merged.pop()\n",
        "    return merged\n",
        "\n",
        "def clean_blank(text):\n",
        "    return re.sub(r'\\s+', ' ', str(text).strip())\n",
        "\n",
        "def get_char_type(char):\n",
        "    if char.isspace():\n",
        "        return \"space\"\n",
        "    elif '가' <= char <= '힣':\n",
        "        return \"hangul\"\n",
        "    elif char.isdigit():\n",
        "        return \"num\"\n",
        "    elif ('A' <= char <= 'Z') or ('a' <= char <= 'z'):\n",
        "        return \"eng\"\n",
        "    else:\n",
        "        return \"symbol\"\n",
        "\n",
        "def chunk_dataframe(df, input_cleaned_col, output_cleaned_col=None):\n",
        "    df = df.copy()\n",
        "    df['chunked_input'] = df[input_cleaned_col].apply(split_sentences)\n",
        "    if output_cleaned_col:\n",
        "        df['chunked_output'] = df[output_cleaned_col].apply(split_sentences)\n",
        "\n",
        "    df_input = df.explode('chunked_input').reset_index(drop=False)\n",
        "    df_input['chunked_input'] = df_input['chunked_input'].apply(clean_blank)\n",
        "\n",
        "    if output_cleaned_col:\n",
        "        df_output = df.explode('chunked_output').reset_index(drop=False)\n",
        "        df_output['chunked_output'] = df_output['chunked_output'].apply(clean_blank)\n",
        "        if not df_input['index'].equals(df_output['index']):\n",
        "            raise ValueError(\"입력과 출력의 explode 결과 인덱스가 일치하지 않습니다.\")\n",
        "        df_input['chunked_output'] = df_output['chunked_output']\n",
        "\n",
        "    df_input = df_input[df_input['chunked_input'] != ''].reset_index(drop=True)\n",
        "    df_input['len'] = df_input['chunked_input'].apply(len)\n",
        "    return df_input\n",
        "\n",
        "def process_files(train_file, test_file, input_col, output_col=None,\n",
        "                  valid_file=None,\n",
        "                  cleaned_input_col=\"cleaned_input\", cleaned_output_col=\"cleaned_output\",\n",
        "                  train_out_csv=\"data/train_chunked.csv\", test_out_csv=\"data/test_chunked.csv\"):\n",
        "\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    if valid_file:\n",
        "        valid_df = pd.read_csv(valid_file)\n",
        "        train_df = pd.concat([train_df, valid_df]).reset_index(drop=True)\n",
        "    test_df = pd.read_csv(test_file)\n",
        "\n",
        "    train_df[cleaned_input_col] = train_df[input_col].apply(clean_text)\n",
        "    test_df[cleaned_input_col] = test_df[input_col].apply(clean_text)\n",
        "    if output_col:\n",
        "        train_df[cleaned_output_col] = train_df[output_col].apply(clean_text)\n",
        "\n",
        "    train_chunked = chunk_dataframe(train_df, cleaned_input_col, cleaned_output_col if output_col else None)\n",
        "    test_chunked = chunk_dataframe(test_df, cleaned_input_col)\n",
        "\n",
        "    train_chunked.to_csv(train_out_csv, index=False)\n",
        "    test_chunked.to_csv(test_out_csv, index=False)\n",
        "\n",
        "def process_standard():\n",
        "    process_files(\n",
        "        train_file=path+\"/train.csv\",\n",
        "        test_file=path+\"test.csv\",\n",
        "        input_col=\"input\",\n",
        "        output_col=\"output\",\n",
        "        cleaned_input_col=\"cleaned_input\",\n",
        "        cleaned_output_col=\"cleaned_output\",\n",
        "        train_out_csv=\"data/train_chunked.csv\",\n",
        "        test_out_csv=\"data/test_chunked.csv\"\n",
        "    )\n",
        "\n",
        "def process_dec1():\n",
        "    process_files(\n",
        "        train_file=path+\"/TRAIN_dec1.csv\",\n",
        "        test_file=path+\"/TEST_dec1.csv\",\n",
        "        input_col=\"input_DEC1\",\n",
        "        output_col=\"output\",\n",
        "        valid_file=path+\"valid_dec1.csv\",\n",
        "        cleaned_input_col=\"cleaned_input_DEC1\",\n",
        "        cleaned_output_col=\"cleaned_output\",\n",
        "        train_out_csv=\"data/train_chunked_dec1.csv\",\n",
        "        test_out_csv=\"data/test_chunked_dec1.csv\"\n",
        "    )\n",
        "\n",
        "def filter_sentence(sentence, allowed_labels={\"hangul\", \"eng\", \"space\", \"num\"}):\n",
        "    sentence = str(sentence)\n",
        "    return \"\".join(char for char in sentence if get_char_type(char) in allowed_labels)\n",
        "\n",
        "def filter_df(df, input_col, output_col=None, new_input_col=\"input\", new_output_col=\"output\"):\n",
        "    df = df.copy()\n",
        "    df[new_input_col] = df[input_col].apply(filter_sentence)\n",
        "    if output_col:\n",
        "        df[new_output_col] = df[output_col].apply(filter_sentence)\n",
        "        return df[[new_input_col, new_output_col]]\n",
        "    return df[[new_input_col]]\n",
        "\n",
        "def process_finetuning(in_train_csv, in_test_csv, out_train_csv, out_test_csv):\n",
        "    train = pd.read_csv(in_train_csv).drop(columns=\"index\", errors='ignore')\n",
        "    test = pd.read_csv(in_test_csv).drop(columns=\"index\", errors='ignore')\n",
        "\n",
        "    train_filtered = filter_df(train, \"chunked_input\", \"chunked_output\", new_input_col=\"input\", new_output_col=\"output\")\n",
        "    test_filtered = filter_df(test, \"chunked_input\", output_col=None, new_input_col=\"input\")\n",
        "\n",
        "    train_filtered.to_csv(out_train_csv, index=False)\n",
        "    test_filtered.to_csv(out_test_csv, index=False)\n",
        "\n",
        "def process_finetuning_original():\n",
        "    process_finetuning(\"data/train_chunked.csv\", \"data/test_chunked.csv\",\n",
        "                         \"data/finetuning_train_orginal.csv\", \"data/finetuning_test_original.csv\")\n",
        "\n",
        "def process_finetuning_mlp():\n",
        "    process_finetuning(\"data/train_chunked_dec1.csv\", \"data/test_chunked_dec1.csv\",\n",
        "                         \"data/finetuning_train_mlp.csv\", \"data/finetuning_test_mlp.csv\")\n",
        "\n",
        "def finetuning_dataset_maker():\n",
        "    process_standard()\n",
        "    process_dec1()\n",
        "    process_finetuning_original()\n",
        "    process_finetuning_mlp()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    finetuning_dataset_maker()"
      ],
      "metadata": {
        "id": "2QbqBs-GVFwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqAB7Y3cfkk5"
      },
      "source": [
        "# Seqmatcher에서 사용할 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVQmk5YAfklA"
      },
      "source": [
        "- electra 결과 문장부호 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqEAF3qDfklN"
      },
      "outputs": [],
      "source": [
        "dec2 = pd.read_csv(path+\"/TEST_dec2_chunked.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYTW5-SYfklN"
      },
      "outputs": [],
      "source": [
        "dec2_dict = {}\n",
        "\n",
        "for idx, text in dec2[\"chunked_input\"].items():\n",
        "    char_list = [(char, get_char_type(char)) for char in text]\n",
        "    dec2_dict[idx] = pd.DataFrame(char_list, columns=[\"Character\", \"Label\"])\n",
        "\n",
        "# 허용할 라벨 리스트\n",
        "allowed_labels = {\"hangul\", \"eng\", \"space\", \"num\"}\n",
        "\n",
        "# 각 row별로 문자 필터링 후 합친 문장을 저장\n",
        "\n",
        "cleaned_dec2 = pd.DataFrame({\n",
        "    \"chunked_input\": [\n",
        "        \"\".join(dec2_dict[idx].loc[dec2_dict[idx][\"Label\"].isin(allowed_labels), \"Character\"])\n",
        "        for idx in dec2_dict\n",
        "    ]\n",
        "})\n",
        "\n",
        "# cc: chunked & cleaned\n",
        "# index는 지움\n",
        "dec2_cc = cleaned_dec2.rename(columns={\"chunked_input\": \"input\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP19E9PXfklO"
      },
      "outputs": [],
      "source": [
        "# chunking시 선공백처리, 후공백처리 (clean_blank)에 따라 문장 분리가 안일어나는 문장 하나가 있음...\n",
        "# 장점 접근성이 나쁘지 않습니다 / 부산역에서 버스 15분 로비가 3층이라 호텔 출입 시 프라이빗 보장이 됩니다\n",
        "# 분리시켜서 최종적으로 4808행을 4809행으로 늘림\n",
        "\n",
        "# 기존 데이터 가져오기\n",
        "data = dec2_cc.copy()\n",
        "\n",
        "# 104번째 행의 값 가져오기\n",
        "original_text = data.iloc[104, 0]\n",
        "\n",
        "# \"장점 접근성이 나쁘지 않습니다\"를 기준으로 분리\n",
        "split_point = \"장점 접근성이 나쁘지 않습니다\"\n",
        "if split_point in original_text:\n",
        "    first_part = split_point\n",
        "    second_part = original_text.replace(split_point, \"\").strip()  # 나머지 부분 가져오기\n",
        "\n",
        "    # 104번째 행 업데이트\n",
        "    data.iloc[104, 0] = first_part\n",
        "\n",
        "    # 이후 행들을 한 칸씩 밀기\n",
        "    new_rows = pd.DataFrame({0: [second_part]})  # 새로운 행 생성\n",
        "    data = pd.concat([data.iloc[:105], new_rows, data.iloc[105:]]).reset_index(drop=True)\n",
        "\n",
        "data.iloc[105, 0] = data.iloc[105, 1]\n",
        "dec2_cc = data.drop(columns=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkomB3vtfklO"
      },
      "outputs": [],
      "source": [
        "dec2_cc.to_csv(path+\"/seqmatcher_ELECTRA.csv\", index=False) # seqmatcher에서 사용할 dec2 청크 문장부호 제거입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrqLFXoifklO"
      },
      "source": [
        "- 원본 데이터 청크와 문장부호 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mj1V9i5fklP"
      },
      "outputs": [],
      "source": [
        "original = pd.read_csv(path+\"/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh_XPkpdfklQ"
      },
      "outputs": [],
      "source": [
        "original[\"cleaned_input\"] = original[\"input\"].apply(clean_text)\n",
        "original_ = original.copy()\n",
        "original_['chunked_input'] = original['cleaned_input'].apply(split_sentences)\n",
        "original_chunked_ = original_.explode('chunked_input')[['chunked_input']].reset_index(drop=False)\n",
        "original_chunked_['chunked_input'] = original_chunked_['chunked_input'].apply(clean_blank)\n",
        "\n",
        "original_chunked_.drop(original_chunked_[original_chunked_['chunked_input'] == ''].index, inplace=True)\n",
        "original_chunked_.reset_index(drop=True, inplace=True)\n",
        "original_chunked_['len'] = original_chunked_['chunked_input'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTHW9-_sfklS"
      },
      "outputs": [],
      "source": [
        "original_dict = {}\n",
        "\n",
        "for idx, text in original_chunked_[\"chunked_input\"].items():\n",
        "    char_list = [(char, get_char_type(char)) for char in text]\n",
        "    original_dict[idx] = pd.DataFrame(char_list, columns=[\"Character\", \"Label\"])\n",
        "\n",
        "allowed_labels = {\"hangul\", \"eng\", \"space\", \"num\"}\n",
        "\n",
        "# 각 row별로 문자 필터링 후 합친 문장을 저장\n",
        "\n",
        "cleaned_original = pd.DataFrame({\n",
        "    \"chunked_input\": [\n",
        "        \"\".join(original_dict[idx].loc[original_dict[idx][\"Label\"].isin(allowed_labels), \"Character\"])\n",
        "        for idx in original_dict\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN1-A91afklT"
      },
      "outputs": [],
      "source": [
        "original_cc = cleaned_original.rename(columns={\"chunked_input\": \"input\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW1jro16fklU"
      },
      "outputs": [],
      "source": [
        "original_cc.to_csv(path+\"/seqmatcher_ORIGINAL.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차 해독_finetuning"
      ],
      "metadata": {
        "id": "8ocJIQcGr1WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning은 Runpod 환경에서 실행됩니다.\n",
        "\n",
        "* OS: ubuntu22.04\n",
        "* CPU: Intel(R) Xeon(R) Platinum 8462Y 16 Core CPU\n",
        "* RAM: 251GB\n",
        "* GPU: A100 SXM\n",
        "* VRAM: 80GB\n",
        "* py3.10, pytorch 2.2.0, cuda12.1.1 <br>\n",
        "깃허브 링크 <br>\n",
        "https://github.com/junseok0913/Noised-Korean-Review-Deobfuscator"
      ],
      "metadata": {
        "id": "r42QTLxHJT_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "p1URYPqJr17y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# cuDNN 관련 설정\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"  # (PyTorch 1.8 이상에서 추천)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "os.environ[\"HF_HOME\"] = path+\"/workspace/huggingface\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = path+\"/workspace/huggingface\""
      ],
      "metadata": {
        "id": "gDWpNXB0xCpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 통합 파이프라인 함수\n",
        "def full_training_pipeline(train_df, output_dir, base_model_path, adapter_model_path, epoch):\n",
        "    # 8-bit BitsAndBytes config (8-bit uses bfloat16)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True\n",
        "    )\n",
        "\n",
        "    model_id = 'beomi/gemma-ko-7b'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0},\n",
        "        cache_dir=\"/workspace/huggingface\"    # <--- volume 지정\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        cache_dir=\"/workspace/huggingface\"    # <--- volume 지정\n",
        "    )\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'right'\n",
        "\n",
        "    train = Dataset.from_pandas(train_df)\n",
        "\n",
        "    def prompting(input, output):\n",
        "        prompt = (\n",
        "            \"<start_of_turn> Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. The number of words and letters per word must be observed.\\n\"\n",
        "            f\"Input: {input}\\n\"\n",
        "            \"<end_of_turn>\\n\"\n",
        "            \"<start_of_turn>Assistant:\\n\"\n",
        "            f\"Output: {output}\"\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def chat_format(row):\n",
        "        prompt = prompting(row[\"input\"], row[\"output\"])\n",
        "        tokens = tokenizer.encode(prompt, truncation=True, max_length=512)\n",
        "        row[\"input_ids\"] = tokens\n",
        "        return row\n",
        "\n",
        "    train = train.map(chat_format, batched=False, num_proc=4)\n",
        "\n",
        "    # LoRA 설정\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"down_proj\", \"up_proj\"\n",
        "        ],\n",
        "        lora_dropout=0.1,\n",
        "        bias='none',\n",
        "        task_type='CAUSAL_LM'\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        seed=42,\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epoch,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=8,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        eval_strategy=\"no\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=50,\n",
        "        warmup_steps=20,\n",
        "        logging_strategy=\"steps\",\n",
        "        learning_rate=2e-4,\n",
        "        group_by_length=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train,\n",
        "        args=training_args,\n",
        "        peft_config=lora_config,\n",
        "        formatting_func=lambda x: x['input_ids']\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # 모델 저장 및 로드\n",
        "    trainer.model.save_pretrained(adapter_model_path)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    cache_dir=path+\"/workspace/huggingface\")\n",
        "\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        adapter_model_path,\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #파일 위치는 실행 환경에 따라 바꿔주세요.\n",
        "\n",
        "    #2차 해독_finetuning_ver1(input:MLP)\n",
        "    train0 = pd.read_csv(path+'/finetuning_train_mlp.csv')\n",
        "    output_dir0=path+\"/results0221\"\n",
        "    base_model_path0 = \"beomi/gemma-ko-7b\"\n",
        "    adapter_model_path0 =  path+\"/workspace/results0221/lora-adapter-epoch3\"\n",
        "\n",
        "    model0 = full_training_pipeline(train0, output_dir0, base_model_path0, adapter_model_path0, epoch=3)\n",
        "    model0.save_pretrained(path+\"/workspace/results0221/gemma-finetuning-epoch3\")\n",
        "    print(\"2차 해독_finetuning_ver1(input:MLP) 완료\")\n",
        "\n",
        "    #2차 해독_finetuning_ver1(input:원본)\n",
        "    train1 = pd.read_csv(path+'finetuning_train_original.csv')\n",
        "    output_dir1=path+\"/results0225\"\n",
        "    base_model_path1 = \"beomi/gemma-ko-7b\"\n",
        "    adapter_model_path1 = path+\"/workspace/results0225/lora-adapter-epoch5\"\n",
        "\n",
        "    model1 = full_training_pipeline(train1, output_dir1, base_model_path1, adapter_model_path1, epoch=5)\n",
        "    model1.save_pretrained(path+\"/workspace/results0225/gemma-finetuning-epoch5\")\n",
        "    print(\"2차 해독_finetuning_ver1(input:원본) 완료\")"
      ],
      "metadata": {
        "id": "6t-0Ip0BIBY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_reviews(FINETUNE_MODEL, BASE_MODEL, test):\n",
        "    finetune_model = AutoModelForCausalLM.from_pretrained(\n",
        "        FINETUNE_MODEL, cache_dir=\"/workspace/huggingface\", device_map={\"\": 0}\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=\"/workspace/huggingface\")\n",
        "\n",
        "    text_gen_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=finetune_model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    restored_reviews = []\n",
        "    total_reviews = len(test)\n",
        "\n",
        "    with tqdm(total=total_reviews, desc=\"Processing\", unit=\"review\") as pbar:\n",
        "        for index, row in test.iterrows():\n",
        "            query = row['input']\n",
        "            prompt = (\n",
        "                \"<start_of_turn> Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. The number of words and letters per word must be observed.\\n\"\n",
        "                f\"Input: {query}\\n\"\n",
        "                \"<end_of_turn>\\n\"\n",
        "                \"<start_of_turn>Assistant:\\n\"\n",
        "                \"Output:\"\n",
        "            )\n",
        "\n",
        "            generated = text_gen_pipeline(\n",
        "                prompt,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                max_new_tokens=len(query),\n",
        "                do_sample=True,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            generated_text = generated[0]['generated_text']\n",
        "            output_start = generated_text.find(\"Output:\")\n",
        "\n",
        "            if output_start != -1:\n",
        "                restored_reviews.append(generated_text[output_start + len(\"Output:\"):].strip())\n",
        "            else:\n",
        "                restored_reviews.append(generated_text.strip())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    return restored_reviews\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  #파일 위치는 실행 환경에 따라 바꿔주세요.\n",
        "\n",
        "  FINETUNE_MODEL0 = path+\"/workspace/results0221/gemma-finetuning-epoch3\"\n",
        "  BASE_MODEL0 = \"beomi/gemma-ko-7b\"\n",
        "  test0 = pd.read_csv(path+'/finetuning_test_mlp.csv')\n",
        "  restored_reviews0 = restore_reviews(FINETUNE_MODEL0, BASE_MODEL0, test0)\n",
        "\n",
        "  FINETUNE_MODEL1 = path+\"/workspace/results0225/2gemma-finetuning-epoch5\"\n",
        "  BASE_MODEL1 = \"beomi/gemma-ko-7b\"\n",
        "  test1 = pd.read_csv(path+'/finetuning_test_original.csv')\n",
        "  restored_reviews1 = restore_reviews(FINETUNE_MODEL1, BASE_MODEL1, test1)"
      ],
      "metadata": {
        "id": "gpUPgpCUI4zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
        "submission = pd.DataFrame()\n",
        "submission['output'] = restored_reviews0\n",
        "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end_of_turn>\")[0])\n",
        "submission.to_csv(path+'/kogemma_0222_raw.csv', index = False, encoding = 'utf-8-sig')"
      ],
      "metadata": {
        "id": "yADhegeD5UXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
        "submission = pd.DataFrame()\n",
        "submission['output'] = restored_reviews1\n",
        "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end_of_turn>\")[0])\n",
        "submission.to_csv(path+'/kogemma_0226_raw.csv', index = False, encoding = 'utf-8-sig')"
      ],
      "metadata": {
        "id": "LXPgY5dj5y1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence Matcher"
      ],
      "metadata": {
        "id": "FOam3kYJDUVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import difflib\n",
        "from jamo import h2j\n",
        "from rapidfuzz import fuzz\n",
        "import re"
      ],
      "metadata": {
        "id": "J6aJW2_rDV9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## koGemma 후처리 함수"
      ],
      "metadata": {
        "id": "DckOWjcYDlCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fuzzy_equal(a: str, b: str, threshold: float) -> bool:\n",
        "    if len(a) != len(b):\n",
        "        return False\n",
        "    return fuzz.ratio(h2j(a), h2j(b)) >= threshold\n",
        "\n",
        "def refine_replace_block(tokens_a, tokens_b, threshold: float):\n",
        "    n, m = len(tokens_a), len(tokens_b)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            if fuzzy_equal(tokens_a[i], tokens_b[j], threshold):\n",
        "                dp[i+1][j+1] = dp[i][j] + 1\n",
        "            else:\n",
        "                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
        "    i, j = n, m\n",
        "    matched = []\n",
        "    while i > 0 and j > 0:\n",
        "        if fuzzy_equal(tokens_a[i-1], tokens_b[j-1], threshold):\n",
        "            matched.append((i-1, j-1))\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        else:\n",
        "            if dp[i-1][j] >= dp[i][j-1]:\n",
        "                i -= 1\n",
        "            else:\n",
        "                j -= 1\n",
        "    matched.reverse()\n",
        "\n",
        "    opcodes = []\n",
        "    a_start, b_start = 0, 0\n",
        "    for a_idx, b_idx in matched:\n",
        "        if a_start < a_idx:\n",
        "            opcodes.append(('delete', a_start, a_idx, b_start, b_start))\n",
        "        opcodes.append(('equal', a_idx, a_idx+1, b_idx, b_idx+1))\n",
        "        a_start = a_idx + 1\n",
        "        b_start = b_idx + 1\n",
        "    if a_start < n:\n",
        "        opcodes.append(('delete', a_start, n, b_start, b_start))\n",
        "    return opcodes\n",
        "\n",
        "def process_pairwise_block(block_org, block_a, block_b, similarity_threshold):\n",
        "    \"\"\"\n",
        "    동일 길이의 토큰 블록에 대해, 원본(block_org)과의 유사도를 비교하여\n",
        "    더 유사한 토큰을 선택합니다.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for org_tok, a_tok, b_tok in zip(block_org, block_a, block_b):\n",
        "        if len(a_tok) == len(b_tok):\n",
        "            score_a = fuzz.ratio(h2j(org_tok), h2j(a_tok))\n",
        "            score_b = fuzz.ratio(h2j(org_tok), h2j(b_tok))\n",
        "            result.append(a_tok if score_a > score_b else b_tok)\n",
        "        else:\n",
        "            result.append(a_tok)\n",
        "    return result\n",
        "\n",
        "def process_replace_block(block_org, block_a, block_b, similarity_threshold):\n",
        "    \"\"\"\n",
        "    replace 구간 처리:\n",
        "      - 토큰 수가 같으면 pairwise 비교,\n",
        "      - 다르면 LCS 기반으로 재세분화(refine_replace_block)를 수행하여 세부 opcodes에 따라 처리.\n",
        "    \"\"\"\n",
        "    if len(block_a) == len(block_b):\n",
        "        return process_pairwise_block(block_org, block_a, block_b, similarity_threshold)\n",
        "    else:\n",
        "        refined_opcodes = refine_replace_block(block_a, block_b, similarity_threshold)\n",
        "        result = []\n",
        "        for tag, i1, i2, j1, j2 in refined_opcodes:\n",
        "            if tag == 'equal':\n",
        "                sub_org = block_org[i1:i2]\n",
        "                sub_a = block_a[i1:i2]\n",
        "                sub_b = block_b[j1:j2]\n",
        "                if len(sub_a) == len(sub_b):\n",
        "                    result.extend(process_pairwise_block(sub_org, sub_a, sub_b, similarity_threshold))\n",
        "                else:\n",
        "                    result.extend(sub_b)\n",
        "            elif tag == 'delete':\n",
        "                result.extend(block_a[i1:i2])\n",
        "            elif tag == 'replace':\n",
        "                result.extend(block_b[j1:j2])\n",
        "        return result\n",
        "\n",
        "def transform_sentence(original: str, sent_a: str, sent_b: str, similarity_threshold=50) -> str:\n",
        "    tokens_org = original.split()\n",
        "    tokens_a = sent_a.split()\n",
        "    tokens_b = sent_b.split()\n",
        "\n",
        "    matcher = difflib.SequenceMatcher(None, tokens_a, tokens_b)\n",
        "    opcodes = matcher.get_opcodes()\n",
        "\n",
        "    result_tokens = []\n",
        "    for tag, i1, i2, j1, j2 in opcodes:\n",
        "        if tag == 'equal':\n",
        "            result_tokens.extend(tokens_b[j1:j2])\n",
        "        elif tag == 'delete':\n",
        "            result_tokens.extend(tokens_a[i1:i2])\n",
        "        elif tag == 'insert':\n",
        "            continue  # insert 구간은 무시\n",
        "        elif tag == 'replace':\n",
        "            block_org = tokens_org[i1:i2]\n",
        "            block_a = tokens_a[i1:i2]\n",
        "            block_b = tokens_b[j1:j2]\n",
        "            result_tokens.extend(process_replace_block(block_org, block_a, block_b, similarity_threshold))\n",
        "\n",
        "    len_diff = len(tokens_a) - len(result_tokens)\n",
        "    if len_diff > 0:\n",
        "        result_tokens.extend(tokens_a[-len_diff:])\n",
        "    elif len_diff < 0:\n",
        "        result_tokens = result_tokens[:len(tokens_a)]\n",
        "\n",
        "    return \" \".join(result_tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    original = (\"특힌 캑쉴 1012횬눈 쌈뮷쉴 뒷펀 예열켠 싫욉귀갸 샴얼 죵때료 토엷한 샥먁한 푸갸 쉬야위 절뱌늚 쨔찌합뉘답\")\n",
        "    sent_a = (\"특히 객실 1012호는 사무실 뒤편 에어컨 실외기가 샤워 초대로 도염한 싼막한 뷰가 시양이 절반을 차취합니다\")\n",
        "    sent_b = (\"특히 객실 1012호는 사무실 뒤편 에어컨 실외기가 샤워 창틀 쪽으로 돌출한 삭막한 뷰가 시야를 차지합니다\")\n",
        "\n",
        "    # original = (\"3밗 옌아켓탁갸 1빡 학꼲 딴 뗄룟 옳걷닸\")\n",
        "    # sent_a = (\"3박 예약했다가 1박 하고 딴 데로 옮겼다\")\n",
        "    # sent_b = (\"3박 예약했다가 1박 하고 다 데로 돌려 받았다\")\n",
        "\n",
        "    original =  (\"쮜겡군 끄 와츙웨 왠 네 물켱는 쥐엊치꽃 냐걋눈 견짐 앍코 본뉘 뀔를 팡 앉에 놓꼬 낚왓단는 쮜껙긔 만뤠 샵창이 먁슴떠끼룰 줆엊터락굡욤\")\n",
        "    sent_a =    (\"치객은 그 와중에 왜 내 물거는 찢어지고 나가는 건지 않고 보니 키를 방 안에 놓고 나왔다는 취객의 만에 사장이 막스더기를 주셨더라고요\")\n",
        "    sent_b =    (\"치킨은 그 와중에 왜 내 물컵은 치워지고 나갔는 건지 알고 보니 키를 방 안에 놓고 나왔다는 치킨의 말에 사장이 마스터키를 주었다고요\")\n",
        "\n",
        "    merged = transform_sentence(original, sent_a, sent_b, similarity_threshold=50)\n",
        "    print(\"[최종 결과]\\n\", merged)\n",
        "    print(\"[문장 A 토큰 수] \", len(sent_a.split()))\n",
        "    print(\"[최종 결과 토큰 수]\", len(merged.split()))\n"
      ],
      "metadata": {
        "id": "uKxYVnQGDo6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uf7ILMYWw0r"
      },
      "source": [
        "## electra 잘못된 문장부호 추론 대체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRuBvvMVWw0s"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST_0328\n",
        "kogemma raw\n",
        "베개에 고무의처럼 얼룩이 있어서 그린 줄 알았는데 커버를 살짝 벗겨보니 누가 침을 흘렸던 베개인지 오염돼서 생긴 얼룩에 커버를 씌운 거였습니다\n",
        "electra\n",
        "베개에 교문의처럼 얼룩이 있어서 그림일 줄 알았는데, 거버를 살짝 벗겨보니 누가 짐을 흐렸던 베개인지 옵염돼서 생긴 얼룩게 커버를 !은 것이었습니다.\n",
        "후처리\n",
        "베개에 고무의처럼 얼룩이 있어서 그림일 줄 알았는데 커버를 살짝 벗겨보니 누가 침을 흘렸던 베개인지 오염돼서 생긴 얼룩에 커버를 은 것이었습니다\n",
        "원본\n",
        "볘캐웨 꽃뮨늬철렴 얽륙익 잊엿셔 끓림민 출 앍얏눈뒈, 켜벌를 삵짝 벋겊봉뉘 눌갉 찜울 흙렸뎐 뻬게윈쥐 옮엄퉤써 쌩낀 열류궤 컵뻘를 씔운 겸씩얻숩뉜타.\n",
        "\n",
        "electra의 경우 한글 글자를 문장기호로 추론하는 경우가 있음.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVqIoZchWw0s"
      },
      "outputs": [],
      "source": [
        "electra = pd.read_csv(path+\"/seqmatcher_ELECTRA.csv\")\n",
        "test = pd.read_csv(path+\"/finetuning_TEST_mlp.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5EN3YAGWw0t"
      },
      "outputs": [],
      "source": [
        "# 공백 기준으로 어절을 나누는 함수\n",
        "def split_sentence(sentence):\n",
        "    return sentence.split()\n",
        "\n",
        "# 어절 길이 비교 후 대체하는 함수\n",
        "def correct_sentence(electra_sentence, test_sentence):\n",
        "    electra_words = split_sentence(electra_sentence)\n",
        "    test_words = split_sentence(test_sentence)\n",
        "\n",
        "    # 최소한의 길이로 맞춰 비교\n",
        "    min_len = min(len(electra_words), len(test_words))\n",
        "\n",
        "    # 어절 길이가 다르면 test의 어절로 교체\n",
        "    corrected_words = [\n",
        "        test_words[i] if len(electra_words[i]) != len(test_words[i]) else electra_words[i]\n",
        "        for i in range(min_len)\n",
        "    ]\n",
        "\n",
        "    # 만약 electra 문장이 짧으면 test 문장 어절 추가\n",
        "    if len(electra_words) < len(test_words):\n",
        "        corrected_words.extend(test_words[min_len:])\n",
        "\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "# Electra 문장 수정 적용\n",
        "electra[\"no_more_nueggimpyo\"] = [\n",
        "    correct_sentence(e_sent, t_sent) for e_sent, t_sent in zip(electra[\"input\"], test[\"input\"])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oemtViDoWw0u"
      },
      "outputs": [],
      "source": [
        "electra[\"no_more_nueggimpyo\"].to_csv(path+\"/seqmatcher2_ELECTRA.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFR4YvmiWw0u"
      },
      "source": [
        "## kogemma 추론의 ZWS 공백문자 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m296CdLWw0u"
      },
      "source": [
        "### kogemma v1 (mlp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HfdAmVr-h9mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jta--P5Ww0u"
      },
      "outputs": [],
      "source": [
        "# 파일 경로\n",
        "file_path = path+\"/kogemma_0222_raw.csv\"\n",
        "\n",
        "# CSV 파일 로드 (모든 열을 문자열로 로드)\n",
        "zws = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "# Zero-Width Space (ZWS, U+200B) 포함 여부 확인\n",
        "zws_mask = zws.applymap(lambda x: \"\\u200b\" in str(x) if pd.notna(x) else False)\n",
        "\n",
        "# ZWS가 포함된 셀들의 위치 찾기\n",
        "zws_cells = [(row, col) for row, col in zip(*zws_mask.to_numpy().nonzero())]\n",
        "zws_count = len(zws_cells)  # ZWS 포함된 셀 개수\n",
        "print(zws_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xOtGqRZWw0v"
      },
      "outputs": [],
      "source": [
        "# 각 셀에서 ZWS 위치를 저장할 새로운 컬럼 추가\n",
        "zws[\"ZWS_positions\"] = zws.applymap(lambda x: [i for i, char in enumerate(str(x)) if char == \"\\u200b\"] if pd.notna(x) else [])\n",
        "\n",
        "# ZWS 제거 함수 정의\n",
        "def remove_zws(cell):\n",
        "    if isinstance(cell, str):  # 문자열인 경우에만 처리\n",
        "        return cell.replace(\"\\u200b\", \"\")\n",
        "    return cell  # 그 외는 그대로 반환\n",
        "\n",
        "# ZWS 제거 적용\n",
        "zws_cleaned = zws.drop(columns=[\"ZWS_positions\"]).applymap(remove_zws)\n",
        "\n",
        "# 새로운 파일로 저장\n",
        "cleaned_file_path = path+\"/kogemma_0222_zws.csv\"\n",
        "zws_cleaned.to_csv(cleaned_file_path, index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhSruFGxWw0v"
      },
      "source": [
        "### kogemma v2 (원본)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-2FNFkvWw0v"
      },
      "outputs": [],
      "source": [
        "# 파일 경로\n",
        "file_path = path+\"/kogemma_0226_raw.csv\n",
        "\n",
        "# CSV 파일 로드 (모든 열을 문자열로 로드)\n",
        "zws = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "# Zero-Width Space (ZWS, U+200B) 포함 여부 확인\n",
        "zws_mask = zws.applymap(lambda x: \"\\u200b\" in str(x) if pd.notna(x) else False)\n",
        "\n",
        "# ZWS가 포함된 셀들의 위치 찾기\n",
        "zws_cells = [(row, col) for row, col in zip(*zws_mask.to_numpy().nonzero())]\n",
        "zws_count = len(zws_cells)  # ZWS 포함된 셀 개수\n",
        "print(zws_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHrgF-5RWw0v"
      },
      "outputs": [],
      "source": [
        "# 각 셀에서 ZWS 위치를 저장할 새로운 컬럼 추가\n",
        "zws[\"ZWS_positions\"] = zws.applymap(lambda x: [i for i, char in enumerate(str(x)) if char == \"\\u200b\"] if pd.notna(x) else [])\n",
        "\n",
        "# ZWS 제거 함수 정의\n",
        "def remove_zws(cell):\n",
        "    if isinstance(cell, str):  # 문자열인 경우에만 처리\n",
        "        return cell.replace(\"\\u200b\", \"\")\n",
        "    return cell  # 그 외는 그대로 반환\n",
        "\n",
        "# ZWS 제거 적용\n",
        "zws_cleaned = zws.drop(columns=[\"ZWS_positions\"]).applymap(remove_zws)\n",
        "\n",
        "# 새로운 파일로 저장\n",
        "cleaned_file_path = path+\"/kogemma_0226_zws.csv\"\n",
        "zws_cleaned.to_csv(cleaned_file_path, index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbkeS64bWw0v"
      },
      "source": [
        "# 데이터 프레임 가공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpKDUYUmWw0w"
      },
      "source": [
        "### kogemma v1 (mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mksc6yMHWw0w"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "original = pd.read_csv(path+\"/seqmatcher_ORIGINAL.csv\")\n",
        "electra = pd.read_csv(path+\"/seqmatcher2_ELECTRA.csv\")\n",
        "electra = electra.rename(columns={\"no_more_nueggimpyo\": \"input\"})\n",
        "kogemma = pd.read_csv(path+\"/kogemma_0222_zws.csv\").rename(columns={\"output\": \"input\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6kem546Ww0w"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터프레임 생성\n",
        "df = pd.DataFrame()\n",
        "for column in electra.columns:\n",
        "    sent_original_list = original[column].astype(str).tolist()\n",
        "    sent_a_list = electra[column].astype(str).tolist()\n",
        "    sent_b_list = kogemma[column].astype(str).tolist()\n",
        "    df[f\"{column}_electra\"] = sent_a_list\n",
        "    df[f\"{column}_kogemma\"] = [transform_sentence(o ,a, b, 50) for o, a, b in zip(sent_original_list, sent_a_list, sent_b_list)]\n",
        "\n",
        "# 결과 저장\n",
        "df.to_csv(path+\"/0227_electra_vs_kogemmaV1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_data = pd.read_csv(path+\"/test_chunked_dec1.csv\")\n",
        "df = pd.read_csv(path+\"/0227_electra_vs_kogemmaV1.csv\")\n",
        "df[\"index\"] = index_data[\"index\"]\n",
        "df_unchunk = df.groupby(\"index\")[\"input_kogemma\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "df_unchunk = df_unchunk.drop(columns=\"index\")\n",
        "df_unchunk.to_csv(path+\"/0227_unchunk_electra_vs_kogemmaV1.csv\", index=False)"
      ],
      "metadata": {
        "id": "TABV7TtTIZq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKtnpQ1Ww0w"
      },
      "source": [
        "### kogemma v2 (원본)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl46BxsyWw0w"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "original = pd.read_csv(path+\"/seqmatcher_ORIGINAL.csv\")\n",
        "electra = pd.read_csv(path+\"/seqmatcher2_ELECTRA.csv\")\n",
        "electra = electra.rename(columns={\"no_more_nueggimpyo\": \"input\"})\n",
        "#kogemma = pd.read_csv(\"data/seqmatcher_kogemma.csv\")\n",
        "kogemma = pd.read_csv(path+\"/kogemma_0226_zws.csv\").rename(columns={\"output\": \"input\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYFU4VSjWw0w"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터프레임 생성\n",
        "df = pd.DataFrame()\n",
        "for column in electra.columns:\n",
        "    sent_original_list = original[column].astype(str).tolist()\n",
        "    sent_a_list = electra[column].astype(str).tolist()\n",
        "    sent_b_list = kogemma[column].astype(str).tolist()\n",
        "    df[f\"{column}_electra\"] = sent_a_list\n",
        "    df[f\"{column}_kogemma\"] = [transform_sentence(o ,a, b, 50) for o, a, b in zip(sent_original_list, sent_a_list, sent_b_list)]\n",
        "\n",
        "# 결과 저장\n",
        "df.to_csv(path+\"/0227_electra_vs_kogemmaV2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k492HoeGWw0y"
      },
      "outputs": [],
      "source": [
        "index_data = pd.read_csv(path+\"/test_chunked_dec1.csv\")\n",
        "df = pd.read_csv(path+\"/0227_electra_vs_kogemmaV2.csv\")\n",
        "df[\"index\"] = index_data[\"index\"]\n",
        "df_unchunk = df.groupby(\"index\")[\"input_kogemma\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "df_unchunk = df_unchunk.drop(columns=\"index\")\n",
        "df_unchunk.to_csv(path+\"/0227_unchunk_electra_vs_kogemmaV2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryH9dbNDWw03"
      },
      "source": [
        "# 문장 부호 붙이기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtHvXd-wWw03"
      },
      "outputs": [],
      "source": [
        "def replace_with_kogemma(test_text, kogemma_text):\n",
        "    \"\"\"\n",
        "    test_text에서 (가-힣, a-z, A-Z, 0-9)만 순회하며\n",
        "    kogemma_text(공백 제거) 글자들을 순서대로 치환.\n",
        "    만약 글자 수가 다르면 알림 메시지를 리턴.\n",
        "    \"\"\"\n",
        "    # 1) kogemma_text에서 공백 제거 -> 각 글자 리스트\n",
        "    kogemma_chars = list(kogemma_text.replace(\" \", \"\"))\n",
        "\n",
        "    # 2) test_text 내 치환 대상 글자 수 세기\n",
        "    test_valid_chars = re.findall(r'[가-힣a-zA-Z0-9]', test_text)\n",
        "    test_valid_count = len(test_valid_chars)\n",
        "    kogemma_count = len(kogemma_chars)\n",
        "\n",
        "    # 3) 글자 수가 다르면 알림만 리턴\n",
        "    #if test_valid_count != kogemma_count:\n",
        "    #   return f\"[글자 수 불일치] test:{test_valid_count}, unchunked:{kogemma_count} - 내용: {test_text}\"\n",
        "\n",
        "    # 4) 한 글자씩 교체 진행\n",
        "    result_chars = []\n",
        "    idx = 0  # kogemma_chars 인덱스\n",
        "    for ch in test_text:\n",
        "        # (가-힣, a-zA-Z, 0-9)이면 교체\n",
        "        if re.match(r'[가-힣a-zA-Z0-9]', ch):\n",
        "            result_chars.append(kogemma_chars[idx])\n",
        "            idx += 1\n",
        "        else:\n",
        "            # 그 외 (공백, 문장부호, 이모티콘 등)은 그대로 둠\n",
        "            result_chars.append(ch)\n",
        "\n",
        "    return \"\".join(result_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUB_uOIoWw04"
      },
      "source": [
        "### kogemma v1 (mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzMZ1HoZWw04"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(path+\"/test.csv\")\n",
        "kogemma_unchunked_df = pd.read_csv(path+\"/0227_unchunk_electra_vs_kogemmaV1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtOxH6uCWw04"
      },
      "outputs": [],
      "source": [
        "# DF를 순회하면서 치환 결과를 새로운 컬럼으로 넣음\n",
        "df_result = pd.DataFrame(columns=['input', 'input_kogemma', 'output'])\n",
        "\n",
        "for i in range(len(test_df)):\n",
        "    t = test_df.loc[i, 'input']\n",
        "    k = kogemma_unchunked_df.loc[i, 'input_kogemma']\n",
        "    replaced_text = replace_with_kogemma(t, k)\n",
        "\n",
        "    df_result.loc[i] = [t, k, replaced_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtGNrtz4Ww04"
      },
      "outputs": [],
      "source": [
        "sample = pd.read_csv(path+\"/sample_submission.csv\")\n",
        "sample[\"output\"] = df_result[\"output\"]\n",
        "sample.to_csv(path+\"/0227_koGemma_v1.csv\", index=False)\n",
        "temp = pd.read_csv(path+\"/test.csv\")\n",
        "sum(len(temp[\"input\"].iloc[i]) != len(sample[\"output\"].iloc[i]) for i in range(len(temp))) # 길이 검증, 0 나와야함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw1FsOIrWw04"
      },
      "source": [
        "### kogemma v2 (원본)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMdQXZL9Ww04"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(path+\"/test.csv\")\n",
        "kogemma_unchunked_df = pd.read_csv(\"data/0227_unchunk_electra_vs_kogemmaV2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5MXbArqWw04"
      },
      "outputs": [],
      "source": [
        "# DF를 순회하면서 치환 결과를 새로운 컬럼으로 넣음\n",
        "df_result = pd.DataFrame(columns=['input', 'input_kogemma', 'output'])\n",
        "\n",
        "for i in range(len(test_df)):\n",
        "    t = test_df.loc[i, 'input']\n",
        "    k = kogemma_unchunked_df.loc[i, 'input_kogemma']\n",
        "    replaced_text = replace_with_kogemma(t, k)\n",
        "\n",
        "    df_result.loc[i] = [t, k, replaced_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_wWTDzWw04"
      },
      "outputs": [],
      "source": [
        "sample = pd.read_csv(path+\"/sample_submission.csv\")\n",
        "sample[\"output\"] = df_result[\"output\"]\n",
        "sample.to_csv(path+\"/0227_koGemma_v2.csv\", index=False)\n",
        "temp = pd.read_csv(\"test.csv\")\n",
        "sum(len(temp[\"input\"].iloc[i]) != len(sample[\"output\"].iloc[i]) for i in range(len(temp))) # 길이 검증, 0 나와야함"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Selection with perplexity computed by Bllossom 8B"
      ],
      "metadata": {
        "id": "lKck7X1QxFoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def calculate_perplexity(sentence: str, model, tokenizer, device: str) -> float:\n",
        "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    input_ids = encodings.input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "    # perplexity\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()"
      ],
      "metadata": {
        "id": "OzwulKI1uUkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "dpo2Kd3ju-bZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym1_gGfE0O4f"
      },
      "outputs": [],
      "source": [
        "data1 = pd.read_csv(path+\"/0227_koGemma_v1.csv\")\n",
        "data2 = pd.read_csv(path+\"/submission_electra.csv\")\n",
        "data3 = pd.read_csv(path+\"/0227_koGemma_v2.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xolVWK180S2V"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVnA7f5nJ4Ej"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence]\n",
        "    merged_sentences = []\n",
        "    temp = \"\"\n",
        "    open_bracket_count = 0\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        open_bracket_count += sentence.count(\"(\")\n",
        "        open_bracket_count -= sentence.count(\")\")\n",
        "        if (len(sentence) <= 15 and i < len(sentences) - 1) or open_bracket_count > 0:\n",
        "            temp += sentence + \" \"\n",
        "        else:\n",
        "            temp += sentence\n",
        "            merged_sentences.append(temp)\n",
        "            temp = \"\"\n",
        "\n",
        "    if temp:\n",
        "        merged_sentences.append(temp.strip())\n",
        "\n",
        "    if len(merged_sentences) > 1:\n",
        "        merged_sentences[-2] += \" \" + merged_sentences[-1]\n",
        "        merged_sentences.pop()\n",
        "    return merged_sentences\n",
        "\n",
        "\n",
        "def clean_and_chunk_data(data):\n",
        "    data['cleaned_output'] = data['output'].apply(clean_text)\n",
        "    data['length'] = data['cleaned_output'].apply(len)\n",
        "    max_length = data['length'].max()\n",
        "    data['chunked_output'] = data['cleaned_output'].apply(split_sentences)\n",
        "    data_chunked = data.explode('chunked_output')[['chunked_output']].reset_index(drop=False)\n",
        "    data_chunked['len'] = data_chunked['chunked_output'].apply(len)\n",
        "    data_chunked.drop(columns=['len'], inplace=True, errors='ignore')\n",
        "\n",
        "    return data_chunked, max_length\n",
        "\n",
        "data1_chunked, max_length1 = clean_and_chunk_data(data1)\n",
        "data2_chunked, max_length2 = clean_and_chunk_data(data2)\n",
        "data3_chunked, max_length3 = clean_and_chunk_data(data3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length1, max_length2, max_length3"
      ],
      "metadata": {
        "id": "0RCvNWtyJkdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOEGDpjLVp1B"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([data1_chunked, data2_chunked[['chunked_output']], data3_chunked[['chunked_output']]], axis =1)\n",
        "data.columns = ['index','kogemma','electra', 'kogemma_v2']\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(path+\"/data_chunked.csv\", index=False)"
      ],
      "metadata": {
        "id": "75uqW1LLJwIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kogemma vs Electra"
      ],
      "metadata": {
        "id": "SQ6QBkUpzWQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Candidates"
      ],
      "metadata": {
        "id": "jpOHEROOsW9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path+\"/data_chunked.csv\")"
      ],
      "metadata": {
        "id": "xyVC3xbdsqC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def generate_candidate_sentences(sentence1: str, sentence2: str) -> list:\n",
        "\n",
        "    tokens1 = sentence1.split()\n",
        "    tokens2 = sentence2.split()\n",
        "\n",
        "    if len(tokens1) != len(tokens2):\n",
        "        raise ValueError(\"두 문장의 어절 수가 동일해야 합니다.\")\n",
        "\n",
        "    candidate_tokens = []\n",
        "    for t1, t2 in zip(tokens1, tokens2):\n",
        "        # 어절이 동일하면 한 가지 선택지만, 다르면 두 가지 선택지를 사용합니다.\n",
        "        candidate_tokens.append([t1] if t1 == t2 else [t1, t2])\n",
        "\n",
        "    # Cartesian product를 사용해 모든 조합의 문장을 생성합니다.\n",
        "    candidates = [\" \".join(tokens) for tokens in product(*candidate_tokens)]\n",
        "    return candidates\n",
        "\n",
        "data['candidate'] = data.apply(lambda row: generate_candidate_sentences(row['kogemma'], row['electra']), axis=1)\n",
        "data['len'] = data.apply(lambda x: len(x['candidate']) if len(x['candidate']) > 1 else 0, axis = 1)\n",
        "data['sent_len'] = data.apply(lambda x: len(x['kogemma'].split()), axis = 1)"
      ],
      "metadata": {
        "id": "KQ0KbSb_sY_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['len'].value_counts()"
      ],
      "metadata": {
        "id": "s3TjpgMcKVZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## perplexity"
      ],
      "metadata": {
        "id": "3lCabxu120c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "KcZbDqu2bEqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text\n",
        "\n",
        "def restore_original_text(original_text, decoded_text):\n",
        "\n",
        "    restored_text = \"\"\n",
        "    decoded_idx = 0  # 해독된 텍스트에서 현재 참조 중인 문자 위치\n",
        "\n",
        "    # 한글, 영어, 숫자, 공백, 문장부호만 허용하는 정규식\n",
        "    valid_char_pattern = re.compile(r'[가-힣a-zA-Z0-9(),.!?\\'\\\"\\[\\]\\{\\}:]')\n",
        "\n",
        "\n",
        "    for char in original_text:\n",
        "        if valid_char_pattern.match(char):\n",
        "            # 유효한 문자일 경우, 해독된 문자로 대체\n",
        "            if decoded_idx < len(decoded_text):\n",
        "                restored_text += decoded_text[decoded_idx]\n",
        "                decoded_idx += 1\n",
        "            else:\n",
        "                # 만약 해독된 텍스트가 끝났다면 원문 문자 그대로 추가\n",
        "                restored_text += char\n",
        "        else:\n",
        "            # 특수문자 및 불완전한 한글은 원문 그대로 유지\n",
        "            restored_text += char\n",
        "\n",
        "    return restored_text\n",
        "\n",
        "def check_double_blanks(text):\n",
        "    return bool(re.search(r' {2,}', text))\n",
        "\n",
        "def check_repeated_last_char(text):\n",
        "    return bool(re.search(r'([가-힣])\\1+$', text))\n",
        "\n",
        "def remove_trailing_unks(text):\n",
        "\n",
        "    cleaned_text = re.sub(r'(<unk>\\s*)+$', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def post_processing(data, data_chunked, result, test_flg = True):\n",
        "  temp = data.copy()\n",
        "  temp_chunked = data_chunked.copy()\n",
        "\n",
        "  temp_chunked['result'] = result\n",
        "  data_recovered = temp_chunked.groupby(\"index\").agg(list).reset_index(drop=True)\n",
        "  if test_flg:\n",
        "    data_recovered.columns = ['kogemma','electra', 'result']\n",
        "    data_recovered['result'] = data_recovered['result'].apply(lambda x: ' '.join(x))\n",
        "  else:\n",
        "    data_recovered.columns = ['input_DEC1','output','input_DEC2']\n",
        "    data_recovered['input_DEC2'] = data_recovered['input_DEC2'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "  temp['input'] = temp.apply(lambda x: clean_blank(x['input']), axis = 1)\n",
        "\n",
        "  temp['result'] = data_recovered['result']\n",
        "  temp['result'] = temp.apply(lambda x: clean_blank(x['result']), axis = 1)\n",
        "  temp['result'] = temp.apply(lambda x: x['result'].replace(\" \",\"\"), axis= 1)\n",
        "\n",
        "  temp['result'] = temp.apply(lambda x: restore_original_text(x['input'],x['result']), axis = 1)\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "lPVTH4RfE7-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison"
      ],
      "metadata": {
        "id": "U51tVtiPbdqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_candidate(candidates: list, model, tokenizer, device: str, sentence1: str, sentence2: str) -> str:\n",
        "    \"\"\"\n",
        "    후보 문장 리스트에서, 후보 수에 따라 다음과 같이 선택.\n",
        "      - 후보가 1개이면: 바로 그 문장을 선택 (펄플렉시티 계산 없이)\n",
        "      - 후보가 200개 이상이면: 두 원본 문장(sentence1, sentence2)만 펄플렉시티를 계산하여 더 낮은 값을 가진 문장을 선택\n",
        "      - 그 외의 경우: 모든 후보 문장에 대해 펄플렉시티를 계산한 후, 가장 낮은 값을 가진 문장을 선택\n",
        "\n",
        "    Returns:\n",
        "        (best_candidate, best_ppl): 선택된 문장과 그 perplexity 값 (계산하지 않은 경우 None)\n",
        "    \"\"\"\n",
        "    if len(candidates) == 1:\n",
        "        # 후보가 단 1개이면, 바로 반환 (펄플렉시티 계산 없이)\n",
        "        best_candidate = candidates[0]\n",
        "        return best_candidate\n",
        "\n",
        "    elif len(candidates) >= 200:\n",
        "        # 후보가 너무 많으면, 원본 두 문장끼리만 펄플렉시티를 계산.\n",
        "        ppl1 = calculate_perplexity(sentence1, model, tokenizer, device)\n",
        "        ppl2 = calculate_perplexity(sentence2, model, tokenizer, device)\n",
        "        best_candidate = sentence1 if ppl1 < ppl2 else sentence2\n",
        "        return best_candidate\n",
        "\n",
        "    else:\n",
        "        # 후보가 200개 미만인 경우: 모든 후보에 대해 펄플렉시티를 계산.\n",
        "        best_candidate = None\n",
        "        best_ppl = float(\"inf\")\n",
        "        for candidate in candidates:\n",
        "            ppl = calculate_perplexity(candidate, model, tokenizer, device)\n",
        "            if ppl < best_ppl:\n",
        "                best_ppl = ppl\n",
        "                best_candidate = candidate\n",
        "        return best_candidate"
      ],
      "metadata": {
        "id": "KxcFbuOzbI3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "data['best_sentence'] = data.progress_apply(lambda x: select_best_candidate(x['candidate'],\n",
        "                                                                            model, tokenizer, device,\n",
        "                                                                            x['kogemma'],\n",
        "                                                                            x['electra']), axis=1)"
      ],
      "metadata": {
        "id": "xnL30nNvbLIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(path+'/test.csv')\n",
        "sub = post_processing(test, data[['index','kogemma','electra']], data['best_sentence'])\n",
        "sub.drop('input', axis = 1, inplace = True)\n",
        "sub.columns = ['ID', 'output']\n",
        "sub.to_csv(path+'/kogemma vs electra(submission).csv', index = False)"
      ],
      "metadata": {
        "id": "UCUwaccz8VfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kogemma vs Kogemma_v2"
      ],
      "metadata": {
        "id": "IK7_j1r5zeO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Candidates"
      ],
      "metadata": {
        "id": "YPSboFLTzeO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path+\"/data_chunked.csv\")"
      ],
      "metadata": {
        "id": "A-w_lGlizeO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def generate_candidate_sentences(sentence1: str, sentence2: str) -> list:\n",
        "\n",
        "    tokens1 = sentence1.split()\n",
        "    tokens2 = sentence2.split()\n",
        "\n",
        "    if len(tokens1) != len(tokens2):\n",
        "        raise ValueError(\"두 문장의 어절 수가 동일해야 합니다.\")\n",
        "\n",
        "    candidate_tokens = []\n",
        "    for t1, t2 in zip(tokens1, tokens2):\n",
        "        # 어절이 동일하면 한 가지 선택지만, 다르면 두 가지 선택지를 사용합니다.\n",
        "        candidate_tokens.append([t1] if t1 == t2 else [t1, t2])\n",
        "\n",
        "    # Cartesian product를 사용해 모든 조합의 문장을 생성합니다.\n",
        "    candidates = [\" \".join(tokens) for tokens in product(*candidate_tokens)]\n",
        "    return candidates\n",
        "\n",
        "data['candidate'] = data.apply(lambda row: generate_candidate_sentences(row['kogemma'], row['kogemma_v2']), axis=1)\n",
        "data['len'] = data.apply(lambda x: len(x['candidate']) if len(x['candidate']) > 1 else 0, axis = 1)\n",
        "data['sent_len'] = data.apply(lambda x: len(x['kogemma'].split()), axis = 1)"
      ],
      "metadata": {
        "id": "byV4Yk01zeO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['len'].value_counts()"
      ],
      "metadata": {
        "id": "gVkFPmMMKpr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison"
      ],
      "metadata": {
        "id": "bwfIcPAszeO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_candidate(candidates: list, model, tokenizer, device: str, sentence1: str, sentence2: str) -> str:\n",
        "    \"\"\"\n",
        "    후보 문장 리스트에서, 후보 수에 따라 다음과 같이 선택.\n",
        "      - 후보가 1개이면: 바로 그 문장을 선택 (펄플렉시티 계산 없이)\n",
        "      - 후보가 200개 이상이면: 두 원본 문장(sentence1, sentence2)만 펄플렉시티를 계산하여 더 낮은 값을 가진 문장을 선택\n",
        "      - 그 외의 경우: 모든 후보 문장에 대해 펄플렉시티를 계산한 후, 가장 낮은 값을 가진 문장을 선택\n",
        "\n",
        "    Returns:\n",
        "        (best_candidate, best_ppl): 선택된 문장과 그 perplexity 값 (계산하지 않은 경우 None)\n",
        "    \"\"\"\n",
        "    if len(candidates) == 1:\n",
        "        # 후보가 단 1개이면, 바로 반환 (펄플렉시티 계산 없이)\n",
        "        best_candidate = candidates[0]\n",
        "        return best_candidate\n",
        "\n",
        "    elif len(candidates) >= 300: # 할만해보여서 256까지 허용\n",
        "        # 후보가 너무 많으면, 원본 두 문장끼리만 펄플렉시티를 계산.\n",
        "        ppl1 = calculate_perplexity(sentence1, model, tokenizer, device)\n",
        "        ppl2 = calculate_perplexity(sentence2, model, tokenizer, device)\n",
        "        best_candidate = sentence1 if ppl1 < ppl2 else sentence2\n",
        "        return best_candidate\n",
        "\n",
        "    else:\n",
        "        # 후보가 300개 미만인 경우: 모든 후보에 대해 펄플렉시티를 계산.\n",
        "        best_candidate = None\n",
        "        best_ppl = float(\"inf\")\n",
        "        for candidate in candidates:\n",
        "            ppl = calculate_perplexity(candidate, model, tokenizer, device)\n",
        "            if ppl < best_ppl:\n",
        "                best_ppl = ppl\n",
        "                best_candidate = candidate\n",
        "        return best_candidate\n"
      ],
      "metadata": {
        "id": "x4_MGSTHzeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "data['best_sentence'] = data.progress_apply(lambda x: select_best_candidate(x['candidate'],\n",
        "                                                                            model, tokenizer, device,\n",
        "                                                                            x['kogemma'],\n",
        "                                                                            x['kogemma_v2']), axis=1)"
      ],
      "metadata": {
        "id": "NJdhYahjzeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text\n",
        "\n",
        "def restore_original_text(original_text, decoded_text):\n",
        "\n",
        "    restored_text = \"\"\n",
        "    decoded_idx = 0  # 해독된 텍스트에서 현재 참조 중인 문자 위치\n",
        "\n",
        "    # 한글, 영어, 숫자, 공백, 문장부호만 허용하는 정규식\n",
        "    valid_char_pattern = re.compile(r'[가-힣a-zA-Z0-9(),.!?\\'\\\"\\[\\]\\{\\}:]')\n",
        "\n",
        "\n",
        "    for char in original_text:\n",
        "        if valid_char_pattern.match(char):\n",
        "            # 유효한 문자일 경우, 해독된 문자로 대체\n",
        "            if decoded_idx < len(decoded_text):\n",
        "                restored_text += decoded_text[decoded_idx]\n",
        "                decoded_idx += 1\n",
        "            else:\n",
        "                # 만약 해독된 텍스트가 끝났다면 원문 문자 그대로 추가\n",
        "                restored_text += char\n",
        "        else:\n",
        "            # 특수문자 및 불완전한 한글은 원문 그대로 유지\n",
        "            restored_text += char\n",
        "\n",
        "    return restored_text\n",
        "\n",
        "def check_double_blanks(text):\n",
        "    return bool(re.search(r' {2,}', text))\n",
        "\n",
        "def check_repeated_last_char(text):\n",
        "    return bool(re.search(r'([가-힣])\\1+$', text))\n",
        "\n",
        "def remove_trailing_unks(text):\n",
        "\n",
        "    cleaned_text = re.sub(r'(<unk>\\s*)+$', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def post_processing(data, data_chunked, result, test_flg = True):\n",
        "  temp = data.copy()\n",
        "  temp_chunked = data_chunked.copy()\n",
        "\n",
        "  # for i in range(len(result)):\n",
        "  #   result[i] = remove_trailing_unks(result[i])\n",
        "\n",
        "  temp_chunked['result'] = result\n",
        "  data_recovered = temp_chunked.groupby(\"index\").agg(list).reset_index(drop=True)\n",
        "  if test_flg:\n",
        "    data_recovered.columns = ['kogemma','kogemma_v2', 'result']\n",
        "    data_recovered['result'] = data_recovered['result'].apply(lambda x: ' '.join(x))\n",
        "  else:\n",
        "    data_recovered.columns = ['input_DEC1','output','input_DEC2']\n",
        "    data_recovered['input_DEC2'] = data_recovered['input_DEC2'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "  temp['input'] = temp.apply(lambda x: clean_blank(x['input']), axis = 1)\n",
        "\n",
        "  temp['result'] = data_recovered['result']\n",
        "  temp['result'] = temp.apply(lambda x: clean_blank(x['result']), axis = 1)\n",
        "  temp['result'] = temp.apply(lambda x: x['result'].replace(\" \",\"\"), axis= 1)\n",
        "\n",
        "  temp['result'] = temp.apply(lambda x: restore_original_text(x['input'],x['result']), axis = 1)\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "zTBnPKf9zeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(path+'/test.csv')\n",
        "sub = post_processing(test, data[['index','kogemma','kogemma_v2']], data['best_sentence'])\n",
        "sub.drop('input', axis = 1, inplace = True)\n",
        "sub.columns = ['ID', 'output']\n",
        "sub.to_csv(path+'/kogemma vs kogemma_v2(submission).csv', index = False)"
      ],
      "metadata": {
        "id": "Mo7omgBUzeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# result1 vs result2"
      ],
      "metadata": {
        "id": "TedDVQ5N5H2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "Qj-gJODv5QGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv(path+'/kogemma vs electra(submission).csv')\n",
        "data2 = pd.read_csv(path+'/kogemma vs kogemma_v2(submission).csv')"
      ],
      "metadata": {
        "id": "3RtTgcXc5Rbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQs_l8zy5l3J"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BOxLnVC5l3J"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # 1. 한국어(가-힣), 영어(a-zA-Z), 숫자(0-9), 공백(\\s) 외 특수문자 제거\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s(),.!?\\'\\\"\\[\\]\\{\\}:]', '', text)\n",
        "\n",
        "    # 2. ㅋㅋ, ㅜㅜ, ㅠㅠ, ㅎㅎ 등과 같은 불완전한 한글 제거\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]', '', text)\n",
        "\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    # 기본적인 문장 분리\n",
        "    sentences = re.split(r'(?<=[.!?])\\s', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence]  # 공백 제거 및 빈 요소 제거\n",
        "\n",
        "    merged_sentences = []\n",
        "    temp = \"\"\n",
        "    open_bracket_count = 0  # 열린 괄호 개수\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # 괄호 개수 체크\n",
        "        open_bracket_count += sentence.count(\"(\")\n",
        "        open_bracket_count -= sentence.count(\")\")\n",
        "\n",
        "        # 문장이 15자 이하이거나, 괄호가 닫히지 않았다면 다음 문장과 결합\n",
        "        if (len(sentence) <= 15 and i < len(sentences) - 1) or open_bracket_count > 0:\n",
        "            temp += sentence + \" \"\n",
        "        else:\n",
        "            temp += sentence\n",
        "            merged_sentences.append(temp)\n",
        "            temp = \"\"  # temp 초기화\n",
        "\n",
        "    if temp:  # 마지막 문장이 남아 있다면 추가\n",
        "        merged_sentences.append(temp.strip())\n",
        "\n",
        "    # 마지막 문장은 반드시 마지막에서 두 번째 문장과 결합\n",
        "    if len(merged_sentences) > 1:\n",
        "        merged_sentences[-2] += \" \" + merged_sentences[-1]\n",
        "        merged_sentences.pop()  # 마지막 문장 제거 (이미 합쳐졌으므로)\n",
        "\n",
        "    return merged_sentences\n",
        "\n",
        "def clean_and_chunk_data(data):\n",
        "    data['cleaned_output'] = data['output'].apply(clean_text)\n",
        "    data['length'] = data['cleaned_output'].apply(len)\n",
        "    max_length = data['length'].max()\n",
        "    data['chunked_output'] = data['cleaned_output'].apply(split_sentences)\n",
        "    data_chunked = data.explode('chunked_output')[['chunked_output']].reset_index(drop=False)\n",
        "    data_chunked['len'] = data_chunked['chunked_output'].apply(len)\n",
        "    data_chunked.drop(columns=['len'], inplace=True, errors='ignore')\n",
        "    return data_chunked, max_length\n",
        "\n",
        "# 각 데이터에 함수 적용\n",
        "data1_chunked, max_length1 = clean_and_chunk_data(data1)\n",
        "data2_chunked, max_length2 = clean_and_chunk_data(data2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([data1_chunked, data2_chunked[['chunked_output']]], axis =1)\n",
        "data.columns = ['index','result1','result2']\n",
        "data.to_csv(path+\"/results_chunked.csv\", index=False)"
      ],
      "metadata": {
        "id": "D2sv0CZ55l3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Candidates"
      ],
      "metadata": {
        "id": "uED-8x9E5H2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path+\"/results_chunked.csv\")"
      ],
      "metadata": {
        "id": "iuNRJMfk5H2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def generate_candidate_sentences(sentence1: str, sentence2: str) -> list:\n",
        "\n",
        "    tokens1 = sentence1.split()\n",
        "    tokens2 = sentence2.split()\n",
        "\n",
        "    if len(tokens1) != len(tokens2):\n",
        "        raise ValueError(\"두 문장의 어절 수가 동일해야 합니다.\")\n",
        "\n",
        "    candidate_tokens = []\n",
        "    for t1, t2 in zip(tokens1, tokens2):\n",
        "        # 어절이 동일하면 한 가지 선택지만, 다르면 두 가지 선택지를 사용합니다.\n",
        "        candidate_tokens.append([t1] if t1 == t2 else [t1, t2])\n",
        "\n",
        "    # Cartesian product를 사용해 모든 조합의 문장을 생성합니다.\n",
        "    candidates = [\" \".join(tokens) for tokens in product(*candidate_tokens)]\n",
        "    return candidates\n",
        "\n",
        "data['candidate'] = data.apply(lambda row: generate_candidate_sentences(row['result1'], row['result2']), axis=1)\n",
        "data['len'] = data.apply(lambda x: len(x['candidate']) if len(x['candidate']) > 1 else 0, axis = 1)\n",
        "data['sent_len'] = data.apply(lambda x: len(x['result1'].split()), axis = 1)"
      ],
      "metadata": {
        "id": "aLSgWvFw5H2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison"
      ],
      "metadata": {
        "id": "nwa8LVxn5H2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_candidate(candidates: list, model, tokenizer, device: str, sentence1: str, sentence2: str) -> str:\n",
        "    \"\"\"\n",
        "    후보 문장 리스트에서, 후보 수에 따라 다음과 같이 선택.\n",
        "      - 후보가 1개이면: 바로 그 문장을 선택 (펄플렉시티 계산 없이)\n",
        "      - 후보가 200개 이상이면: 두 원본 문장(sentence1, sentence2)만 펄플렉시티를 계산하여 더 낮은 값을 가진 문장을 선택\n",
        "      - 그 외의 경우: 모든 후보 문장에 대해 펄플렉시티를 계산한 후, 가장 낮은 값을 가진 문장을 선택\n",
        "\n",
        "    Returns:\n",
        "        (best_candidate, best_ppl): 선택된 문장과 그 perplexity 값 (계산하지 않은 경우 None)\n",
        "    \"\"\"\n",
        "    if len(candidates) == 1:\n",
        "        # 후보가 단 1개이면, 바로 반환 (펄플렉시티 계산 없이)\n",
        "        best_candidate = candidates[0]\n",
        "        return best_candidate\n",
        "\n",
        "    elif len(candidates) >= 2000: # 할만해보여서 1024까지 허용\n",
        "        # 후보가 너무 많으면, 원본 두 문장끼리만 펄플렉시티를 계산.\n",
        "        ppl1 = calculate_perplexity(sentence1, model, tokenizer, device)\n",
        "        ppl2 = calculate_perplexity(sentence2, model, tokenizer, device)\n",
        "        best_candidate = sentence1 if ppl1 < ppl2 else sentence2\n",
        "        return best_candidate\n",
        "\n",
        "    else:\n",
        "        # 후보가 2000개 미만인 경우: 모든 후보에 대해 펄플렉시티를 계산.\n",
        "        best_candidate = None\n",
        "        best_ppl = float(\"inf\")\n",
        "        for candidate in candidates:\n",
        "            ppl = calculate_perplexity(candidate, model, tokenizer, device)\n",
        "            if ppl < best_ppl:\n",
        "                best_ppl = ppl\n",
        "                best_candidate = candidate\n",
        "        return best_candidate\n"
      ],
      "metadata": {
        "id": "J0rD4BxC5H2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "data['best_sentence'] = data.progress_apply(lambda x: select_best_candidate(x['candidate'],\n",
        "                                                                            model, tokenizer, device,\n",
        "                                                                            x['result1'],\n",
        "                                                                            x['result2']), axis=1)"
      ],
      "metadata": {
        "id": "OjyAcB0p5H2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_blank(text):\n",
        "    text = text.strip()  # 좌우 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 변환\n",
        "    return text\n",
        "\n",
        "def restore_original_text(original_text, decoded_text):\n",
        "\n",
        "    restored_text = \"\"\n",
        "    decoded_idx = 0  # 해독된 텍스트에서 현재 참조 중인 문자 위치\n",
        "\n",
        "    # 한글, 영어, 숫자, 공백, 문장부호만 허용하는 정규식\n",
        "    valid_char_pattern = re.compile(r'[가-힣a-zA-Z0-9(),.!?\\'\\\"\\[\\]\\{\\}:]')\n",
        "\n",
        "\n",
        "    for char in original_text:\n",
        "        if valid_char_pattern.match(char):\n",
        "            # 유효한 문자일 경우, 해독된 문자로 대체\n",
        "            if decoded_idx < len(decoded_text):\n",
        "                restored_text += decoded_text[decoded_idx]\n",
        "                decoded_idx += 1\n",
        "            else:\n",
        "                # 만약 해독된 텍스트가 끝났다면 원문 문자 그대로 추가\n",
        "                restored_text += char\n",
        "        else:\n",
        "            # 특수문자 및 불완전한 한글은 원문 그대로 유지\n",
        "            restored_text += char\n",
        "\n",
        "    return restored_text\n",
        "\n",
        "def check_double_blanks(text):\n",
        "    return bool(re.search(r' {2,}', text))\n",
        "\n",
        "def check_repeated_last_char(text):\n",
        "    return bool(re.search(r'([가-힣])\\1+$', text))\n",
        "\n",
        "def remove_trailing_unks(text):\n",
        "\n",
        "    cleaned_text = re.sub(r'(<unk>\\s*)+$', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def post_processing(data, data_chunked, result, test_flg = True):\n",
        "  temp = data.copy()\n",
        "  temp_chunked = data_chunked.copy()\n",
        "\n",
        "  # for i in range(len(result)):\n",
        "  #   result[i] = remove_trailing_unks(result[i])\n",
        "\n",
        "  temp_chunked['result'] = result\n",
        "  data_recovered = temp_chunked.groupby(\"index\").agg(list).reset_index(drop=True)\n",
        "  if test_flg:\n",
        "    data_recovered.columns = ['result1','result2', 'result']\n",
        "    data_recovered['result'] = data_recovered['result'].apply(lambda x: ' '.join(x))\n",
        "  else:\n",
        "    data_recovered.columns = ['input_DEC1','output','input_DEC2']\n",
        "    data_recovered['input_DEC2'] = data_recovered['input_DEC2'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "  temp['input'] = temp.apply(lambda x: clean_blank(x['input']), axis = 1)\n",
        "\n",
        "  temp['result'] = data_recovered['result']\n",
        "  temp['result'] = temp.apply(lambda x: clean_blank(x['result']), axis = 1)\n",
        "  temp['result'] = temp.apply(lambda x: x['result'].replace(\" \",\"\"), axis= 1)\n",
        "\n",
        "  temp['result'] = temp.apply(lambda x: restore_original_text(x['input'],x['result']), axis = 1)\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "Q1zuZdqG5H2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(path+'/test.csv')\n",
        "sub = post_processing(test, data[['index','result1','result2']], data['best_sentence'])\n",
        "sub.drop('input', axis = 1, inplace = True)\n",
        "sub.columns = ['ID', 'output']\n",
        "sub.to_csv(path+'/result1 vs result2(submission).csv', index = False)"
      ],
      "metadata": {
        "id": "C-QuhlZ-5H2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 제출\n",
        "아래 세 결과 중에 `kogemma_v1 vs kogemma_v2`로 결정\n",
        "- kogemma vs electra (result1)\n",
        "- kogemma_v1 vs kogemma_v2 (result2)\n",
        "- result1 vs result2"
      ],
      "metadata": {
        "id": "22WOiKyUG9Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble2 = pd.read_csv(path+'/kogemma vs kogemma_v2(submission).csv')"
      ],
      "metadata": {
        "id": "7clexndpHc88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble2.to_csv(path+'/final_submission.csv', index = False)"
      ],
      "metadata": {
        "id": "6xhWmvYDHc88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}